{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goal\n",
    "- Find false innocents, could be through:\n",
    "    - connections with many criminal nodes?\n",
    "    - connection to a big criminal hub?\n",
    "\n",
    "# Assumptions\n",
    "- People connected to a crime as suspect or suspect/victim will be considered as being guilty of the crime they are connected to\n",
    "- \n",
    "\n",
    "## Node categorisation\n",
    "- criminal nodes: nodes that have been a suspect in a acrime at least once\n",
    "- innocent nodes: nodes that have been only victims and/or witness\n",
    "\n",
    "## Suspect role\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Work plan\n",
    "Research question: is it possible to detect criminals that were not previously caught? - \"criminal\" disguised as \"innocent\"\n",
    "\n",
    "- Does a strong link prediction between a criminal and an innocent suggests that the innocent might be a criminal in disguise?\n",
    "- Compare the results from link prediction and community detection\n",
    "- If they are in the same community? Is that an even stronger indication of a possible criminal?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import statistics as stats\n",
    "import numpy as np\n",
    "\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('seaborn version', sns.__version__)\n",
    "print('pandas version', pd.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "al = '..//Data//out.moreno_crime_crime'\n",
    "gender = '..//Data//ent.moreno_crime_crime.person.sex'\n",
    "name = '..//Data//ent.moreno_crime_crime.person.name'\n",
    "role = '..//Data//rel.moreno_crime_crime.person.role'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjancency list as DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al = pd.read_csv(al, sep=\" \", names=['person', 'crime'], index_col=False)\n",
    "df_al['person'] = 'p' + df_al['person'].astype(str)\n",
    "df_al['crime'] = 'c' + df_al['crime'].astype(str)\n",
    "df_al.head(3)\n",
    "df_al.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gender Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = pd.read_csv(gender, sep=\" \", header=None, names=['gender'])\n",
    "df_gender['person'] = 'p' + df_gender.index.astype(str)\n",
    "df_gender.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_name = pd.read_csv(name, sep=\" \", header=None, names=['name'])\n",
    "df_name['person'] = 'p' + df_name.index.astype(str)\n",
    "df_name.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Role Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_role = pd.read_csv(role, sep=\",\", header=None, names=['role'])\n",
    "# df_role.head(3)\n",
    "df_role.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join adjancency list with role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al_roles = df_al.join(df_role)\n",
    "df_al_roles.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_al_roles[df_al_roles['person'] == 'p815']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group bys gives us the same as degree distribution\n",
    "# df_al_roles.groupby(by='person', dropna=False).count()\n",
    "# df_al_roles.groupby(by=['crime', 'role'], dropna=False).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic data stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following are used to create the graph\n",
    "people = df_al['person'].unique()\n",
    "crimes = df_al['crime'].unique()\n",
    "roles = df_role['role'].unique()\n",
    "\n",
    "# print stats\n",
    "print('Number of people:', len(people))\n",
    "print('Number of crimes:', len(crimes))\n",
    "print('Number of roles:', len(roles))\n",
    "print('Number of edges:', len(df_al_roles))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Breakdown of roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_role.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# G=nx.from_pandas_dataframe(df_al_roles, 0, 'b', ['weight', 'cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create networkx graph\n",
    "G = nx.Graph()\n",
    "\n",
    "# # add nodes\n",
    "for i in range(len(people)):\n",
    "    G.add_node(people[i], name=df_name['name'][i], gender=df_gender['gender'][i], bipartite=0)\n",
    "\n",
    "for i in range(len(crimes)):\n",
    "    G.add_node(crimes[i], bipartite=1)\n",
    "\n",
    "# # add edges\n",
    "for i in range(len(df_al)):\n",
    "    G.add_edge(df_al_roles['person'][i], df_al_roles['crime'][i], role=df_al_roles['role'][i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assign node status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code from Dee\n",
    "# Initialize a dictionary based on people nodes to keep track of all roles per node\n",
    "p_nodes = {el:[] for el in people}\n",
    "\n",
    "# Add all the edge attributes to a dictionary of people nodes\n",
    "for key,value in nx.get_edge_attributes(G, 'role').items():\n",
    "    for part in key:\n",
    "        if part in people:\n",
    "            p_nodes[part].append(value)\n",
    "\n",
    "# print(p_nodes['p1']) # List of all roles from p1\n",
    "\n",
    "# Initialize a dictionary to keep track of who is a \"criminal\"\n",
    "status = {el:[] for el in people}\n",
    "    \n",
    "# Loop through all roles per node, and deem them criminals if ever they have been a suspect\n",
    "for key in p_nodes:\n",
    "    if 'Suspect' in p_nodes[key]:\n",
    "        status[key] = \"criminal\"\n",
    "    elif 'Victim Suspect' in p_nodes[key]:\n",
    "        status[key] = \"criminal\"\n",
    "    else:\n",
    "        status[key] = \"innocent\"\n",
    "\n",
    "print(status['p336']) # Verify that p1 is deemed \"criminal\"\n",
    "\n",
    "# Convert to pandas df\n",
    "criminals_df = pd.DataFrame(status.items(), columns=['node', 'criminal_status'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add node status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through rows in the data frame and add the attribute of Criminal Status\n",
    "for index, row in criminals_df.iterrows():\n",
    "    #print(row['node'])\n",
    "    G.nodes[row['node']]['criminal_status'] = row['criminal_status']\n",
    "\n",
    "print(nx.get_node_attributes(G, 'criminal_status')['p1']) # check name of person 'p1' = 'Criminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get top and bottom nodes for projection and plotting\n",
    "people_nodes = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "crime_nodes = set(G) - people_nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only biggest component\n",
    "# pos = nx.spring_layout(G)\n",
    "# posB = nx.bipartite_layout(G, people_nodes)\n",
    "G_draw = nx.draw_spring(G,node_size=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get degree of all nodes\n",
    "Node degreee of **people** nodes if the number of crimes they were involved in.  \n",
    "Node degree of **crime** nodes is the number of people involved in the crime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dict with all node degrees to add as attribute\n",
    "node_degrees = dict()\n",
    "\n",
    "# Creating dict for each node type\n",
    "people_degrees = dict()\n",
    "crimes_degrees = dict()\n",
    "\n",
    "# for loop to populate dicts above\n",
    "for node in G.nodes:\n",
    "    # print(G.edges(node, data=True))\n",
    "    node_degrees[node] = G.degree(node)\n",
    "    if node.startswith('p') == True:\n",
    "        people_degrees[node] = G.degree(node)\n",
    "    else:\n",
    "        crimes_degrees[node] = G.degree(node)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gender = pd.read_csv(gender, sep=\" \", header=None, names=['gender'])\n",
    "df_gender['person'] = 'p' + df_gender.index.astype(str)\n",
    "df_gender.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add node degree as node attribute in graph G\n",
    "nx.set_node_attributes(G, node_degrees, \"node_degree\")\n",
    "\n",
    "# and check it worked\n",
    "# nx.get_node_attributes(G, 'node_degree')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a dataframe including all node attributes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from https://stackoverflow.com/a/50775962\n",
    "# make pandas dataframe from graph with node attributes\n",
    "df_G = pd.DataFrame.from_dict(dict(G.nodes(data=True)), orient='index')\n",
    "df_G\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G.loc[['p815']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G['criminal_status'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection on people"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G_proj = bipartite.weighted_projected_graph(G, people_nodes, ratio=False)\n",
    "list(G_proj.edges(data=True))[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Num. of nodes: {} \\nNum. of edges: {} \\nIs bipartite? {} \\nIs connected? {}'.format(\n",
    "        G_proj.number_of_nodes(), \n",
    "        G_proj.number_of_edges(), \n",
    "        nx.is_bipartite(G_proj),\n",
    "        nx.is_connected(G_proj)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = list(nx.get_edge_attributes(G_proj, 'weight').values())\n",
    "\n",
    "# plot weights\n",
    "plt.hist(weights, bins = 10, log=True)\n",
    "plt.xticks([1,2,3,4,5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels, counts = np.unique(weights, return_counts=True)\n",
    "plt.bar(labels, counts, align='center', log=True)\n",
    "plt.ylabel('Count')\n",
    "plt.xlabel('Crimes Shared Between People')\n",
    "plt.show()\n",
    "#plt.savefig('../Figures/CrimesBetweenPeople.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "People of interest. Pairs that are connected by 5 crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_w = list(G_proj.edges(data=True))\n",
    "\n",
    "for i in edge_w:\n",
    "    if i[2]['weight'] > 2:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get adjacency matrix\n",
    "A = nx.adjacency_matrix(G_proj, weight='weight')\n",
    "A = A.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A[A > 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot adjacency matrix\n",
    "plt.title('Adjacency Matrix')\n",
    "# plt.imshow(A, cmap='Greys', markersize = 3)\n",
    "# plt.show()\n",
    "# plt.figure(figsize=(8,8))\n",
    "plt.spy(A, markersize = 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot graph visualisation\n",
    "nx.draw_spring(G_proj, node_size=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Link prediction on projected graph\n",
    "from: https://www.networkatlas.eu/exercise.htm?c=20&e=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linkpred_pa = list(nx.preferential_attachment(G_proj))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort in decreasing score\n",
    "linkpred_pa.sort(key = lambda tup: tup[2], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top 10\n",
    "for edge_score in linkpred_pa[:10]:\n",
    "   print(edge_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare different link prediction methods\n",
    "Code from: https://www.networkatlas.eu/exercise.htm?c=20&e=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the other link prediction methods implemented in networkx\n",
    "linkpred_ja = list(nx.jaccard_coefficient(G_proj))\n",
    "linkpred_aa = list(nx.adamic_adar_index(G_proj))\n",
    "linkpred_ra = list(nx.resource_allocation_index(G_proj))\n",
    "\n",
    "linkpred_ja.sort(key = lambda tup: tup[2], reverse = True)\n",
    "linkpred_aa.sort(key = lambda tup: tup[2], reverse = True)\n",
    "linkpred_ra.sort(key = lambda tup: tup[2], reverse = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print top predictions\n",
    "print(\"PrefAtt\\tJaccard\\tAdamAd\\tResAll\")\n",
    "for i in range(5):\n",
    "   print(\"%s\\t%s\\t%s\\t%s\" % (linkpred_pa[i], linkpred_ja[i], linkpred_aa[i], linkpred_ra[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linkpred = pd.DataFrame(linkpred_ra, columns=['node1', 'node2', 'resource_allocation_score'])\n",
    "df_linkpred['node1_status'] = df_linkpred['node1'].map(status)\n",
    "df_linkpred['node2_status'] = df_linkpred['node2'].map(status)\n",
    "# df_linkpred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linkpred_status = df_linkpred[df_linkpred['resource_allocation_score'] != 0]\n",
    "df_linkpred_status.sort_values(by='resource_allocation_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_G.loc['p336'])\n",
    "print(df_G.loc['p815'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_G.loc['p228'])\n",
    "print(df_G.loc['p514'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_G.loc['p301'])\n",
    "print(df_G.loc['p815'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_G.loc['p797'])\n",
    "print(df_G.loc['p293'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df_G.loc['p155'])\n",
    "print(df_G.loc['p269'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_linkpred[df_linkpred['resource_allocation_score'] >= .5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_linkpred[df_linkpred['resource_allocation_score'] > 0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.displot(df_linkpred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G.iloc[410,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_G.iloc[282,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Link prediction experiment\n",
    "from: https://www.networkatlas.eu/exercise.htm?c=22&e=1\n",
    "\n",
    "Divide this network into train and test sets using a ten-fold cross validation scheme. Draw its confusion matrix after applying a jaccard link prediction to it. Use 0.5 as you cutoff score: scores equal to or higher than 0.5 are predicted to be an edge, anything lower is predicted to be a non-edge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data as a pandas dataframe\n",
    "# df1 = pd.read_csv(\"data.txt\", sep = \" \", header = None, names = (\"src\", \"trg\"))\n",
    "# df1[\"target\"] = 1\n",
    "# df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = nx.to_pandas_edgelist(G_proj)\n",
    "df[\"target2\"] = 1\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a ten fold split\n",
    "kf = KFold(n_splits = 10, shuffle = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each fold generates the graph we need for training link prediction and the dataframe\n",
    "# we use to test it.\n",
    "y_true = []\n",
    "y_pred = []\n",
    "for train_index, test_index in kf.split(df):\n",
    "   # Generate the train graph\n",
    "   G_train = nx.from_pandas_edgelist(df.loc[train_index], source = \"source\", target = \"target\")\n",
    "   # Make a dataframe with the results of the jaccard prediction\n",
    "   score = pd.DataFrame(list(nx.jaccard_coefficient(G_train)), columns = (\"source\", \"target\", \"score\"))\n",
    "   # Merge with the test set\n",
    "   df_test = df.loc[test_index].merge(score, on = [\"source\", \"target\"], how = \"outer\").fillna(0)\n",
    "   # Threshold the results with the cutoff\n",
    "   df_test[\"prediction\"] = df_test[\"score\"] >= 0.5\n",
    "   y_true += list(df_test[\"target2\"])\n",
    "   y_pred += list(df_test[\"prediction\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the confusion matrix, which has this shape:\n",
    "# [[ TN, FN ],\n",
    "#  [ FP, TP ]]\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the methods\n",
    "from: https://www.networkatlas.eu/exercise.htm?c=22&e=2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import roc_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[train_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pas = pd.DataFrame()\n",
    "jas = pd.DataFrame()\n",
    "aas = pd.DataFrame()\n",
    "ras = pd.DataFrame()\n",
    "\n",
    "for train_index, test_index in kf.split(df):\n",
    "   G = nx.from_pandas_edgelist(df.loc[train_index], source = \"source\", target = \"target\")\n",
    "   pa = pd.DataFrame(list(nx.preferential_attachment(G)), columns = (\"source\", \"target\", \"score\"))\n",
    "   ja = pd.DataFrame(list(nx.jaccard_coefficient(G)), columns = (\"source\", \"target\", \"score\"))\n",
    "   aa = pd.DataFrame(list(nx.adamic_adar_index(G)), columns = (\"source\", \"target\", \"score\"))\n",
    "   ra = pd.DataFrame(list(nx.resource_allocation_index(G)), columns = (\"source\", \"target\", \"score\"))\n",
    "   # Now we keep all scores, because we want to plot the full ROC curve rather than a simple confusion matrix\n",
    "   pas = pd.concat([pas, df.loc[test_index].merge(pa, on = [\"source\", \"target\"], how = \"outer\").fillna(0)])\n",
    "   jas = pd.concat([jas, df.loc[test_index].merge(ja, on = [\"source\", \"target\"], how = \"outer\").fillna(0)])\n",
    "   aas = pd.concat([aas, df.loc[test_index].merge(aa, on = [\"source\", \"target\"], how = \"outer\").fillna(0)])\n",
    "   ras = pd.concat([ras, df.loc[test_index].merge(ra, on = [\"source\", \"target\"], how = \"outer\").fillna(0)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to look at specific people\n",
    "# ras[ras['source'] == 'p425']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = ras\n",
    "mod[mod['score'] > 0].sort_values(by='score', ascending=False).head(15) #these have existing edges?\n",
    "\n",
    "mod[mod['weight'] == 0].sort_values(by='score', ascending=False).head(15) #eliminating existing edges shows multiple edges for the same pairs of nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we draw the ROCs\n",
    "fpr_pa, tpr_pa, thresholds = roc_curve(pas[\"target2\"], pas[\"score\"])\n",
    "fpr_ja, tpr_ja, thresholds = roc_curve(jas[\"target2\"], jas[\"score\"])\n",
    "fpr_aa, tpr_aa, thresholds = roc_curve(aas[\"target2\"], aas[\"score\"])\n",
    "fpr_ra, tpr_ra, thresholds = roc_curve(ras[\"target2\"], ras[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we plot the random classifier performance\n",
    "plt.plot([0, 1], [0, 1], color = 'gray', lw = 1)\n",
    "# Then the actual classifiers\n",
    "plt.plot(fpr_pa, tpr_pa, label = \"PA\")\n",
    "plt.plot(fpr_ja, tpr_ja, label = \"JA\")\n",
    "plt.plot(fpr_aa, tpr_aa, label = \"AA\")\n",
    "plt.plot(fpr_ra, tpr_ra, label = \"RA\", alpha=.5)\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the AUCs\n",
    "print(\"PA's AUC: %1.4f\" % auc(fpr_pa, tpr_pa))\n",
    "print(\"JA's AUC: %1.4f\" % auc(fpr_ja, tpr_ja))\n",
    "print(\"AA's AUC: %1.4f\" % auc(fpr_aa, tpr_aa))\n",
    "print(\"RA's AUC: %1.4f\" % auc(fpr_ra, tpr_ra))\n",
    "\n",
    "# Resource allocation works best!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate precision, recall, and F1-score for the four link predictors\n",
    "from: https://www.networkatlas.eu/exercise.htm?c=22&e=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get only top 10% of scores as actual predictions\n",
    "pas[\"prediction\"] = pas[\"score\"].rank(pct = True) >= 0.9\n",
    "jas[\"prediction\"] = jas[\"score\"].rank(pct = True) >= 0.9\n",
    "aas[\"prediction\"] = aas[\"score\"].rank(pct = True) >= 0.9\n",
    "ras[\"prediction\"] = ras[\"score\"].rank(pct = True) >= 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now we calculate our quality measures\n",
    "prec_pa, recall_pa, f1_pa, _ = precision_recall_fscore_support(pas[\"target2\"], pas[\"prediction\"], average = \"binary\")\n",
    "prec_ja, recall_ja, f1_ja, _ = precision_recall_fscore_support(jas[\"target2\"], jas[\"prediction\"], average = \"binary\")\n",
    "prec_aa, recall_aa, f1_aa, _ = precision_recall_fscore_support(aas[\"target2\"], aas[\"prediction\"], average = \"binary\")\n",
    "prec_ra, recall_ra, f1_ra, _ = precision_recall_fscore_support(ras[\"target2\"], ras[\"prediction\"], average = \"binary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Let's figure out who performs best:\n",
    "print(\"PA's precision = %1.4f, recall = %1.4f, F1 = %1.4f\" % (prec_pa, recall_pa, f1_pa))\n",
    "print(\"JA's precision = %1.4f, recall = %1.4f, F1 = %1.4f\" % (prec_ja, recall_ja, f1_ja))\n",
    "print(\"AA's precision = %1.4f, recall = %1.4f, F1 = %1.4f\" % (prec_aa, recall_aa, f1_aa))\n",
    "print(\"RA's precision = %1.4f, recall = %1.4f, F1 = %1.4f\" % (prec_ra, recall_ra, f1_ra))\n",
    "\n",
    "# Resource allocation is both the most precise and complete, and thus has also the highest F1 score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Draw the precision-recall curves of the four link predictors\n",
    "from: https://www.networkatlas.eu/exercise.htm?c=22&e=4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's draw the precision-recall curve! Not much different from ROC\n",
    "pr_pa, rec_pa, thresholds = precision_recall_curve(pas[\"target2\"], pas[\"score\"])\n",
    "pr_ja, rec_ja, thresholds = precision_recall_curve(jas[\"target2\"], jas[\"score\"])\n",
    "pr_aa, rec_aa, thresholds = precision_recall_curve(aas[\"target2\"], aas[\"score\"])\n",
    "pr_ra, rec_ra, thresholds = precision_recall_curve(ras[\"target2\"], ras[\"score\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember that here the random classifier is not the 45 degree line, so let's just plot the curves\n",
    "# I use a logarithmic x axis for convenience.\n",
    "plt.plot(rec_pa, pr_pa, label = \"PA\")\n",
    "plt.plot(rec_ja, pr_ja, label = \"JA\")\n",
    "plt.plot(rec_aa, pr_aa, label = \"AA\")\n",
    "plt.plot(rec_ra, pr_ra, label = \"RA\")\n",
    "plt.legend(loc = \"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's print the AUCs\n",
    "print(\"PA's AUC: %1.4f\" % auc(rec_pa, pr_pa))\n",
    "print(\"JA's AUC: %1.4f\" % auc(rec_ja, pr_ja))\n",
    "print(\"AA's AUC: %1.4f\" % auc(rec_aa, pr_aa))\n",
    "print(\"RA's AUC: %1.4f\" % auc(rec_ra, pr_ra))\n",
    "\n",
    "# We already knew that resource allocation works best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82a91a22060084f3e8dbe72ba49a002eddd687e7ed0e7d249946fe1b995c1ddc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
