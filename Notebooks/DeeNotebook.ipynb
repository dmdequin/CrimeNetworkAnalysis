{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "intelligent-instrument",
   "metadata": {},
   "source": [
    "# Network Analysis Project\n",
    "## Crime Network Analysis\n",
    "Professor: Michele Coscia\n",
    "\n",
    "This notebook contains all of the code developed for the Network Analysis Course at ITU. We will be using...**finish**\n",
    "\n",
    "**Add Description**\n",
    "\n",
    "Group 10:  \n",
    "Carl August Wismer ([cwis@itu.dk](mailto:cwis@itu.dk))  <br>\n",
    "Crisanna Cornish ([ccor@itu.dk](mailto:ccor@itu.dk))  <br>\n",
    "Danielle Dequin ([ddeq@itu.dk](mailto:ddeq@itu.dk))  <br>\n",
    "Maria Do Carmo Madeira Santos Silva Passos de Sousa ([mdom@itu.dk](mailto:mdom@itu.dk))  <br>\n",
    "Moneeca Abru Iftikhar Latif ([abml@itu.dk](mailto:abml@itu.dk))  <br>\n",
    "Sabrina Fonseca Pereira ([sabf@itu.dk](mailto:sabf@itu.dk))  <br>\n",
    "\n",
    "Created: 27-09-2021  \n",
    "Last Modified: 19-11-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vanilla-digest",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "The data are a network of associations among suspects, victims, and/or witnesses involved in crimes in St. Louis in the 1990s. Data are derived from police records, via snowball sampling from five initial homicides. Left nodes are people, right nodes are crime events, and edges connect people to particular crimes events they were associated with. Metadata includes names, genders, and roles (suspects, victims, and/or witnesses).\n",
    "\n",
    "This is an undirected, unweighted, bipartite network with 1380 nodes and 1476 edges.\n",
    "\n",
    "Data can be downloaded [here](http://konect.cc/networks/moreno_crime/) or [here](https://networks.skewed.de/net/crime)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "still-sailing",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sound-replacement",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress\n",
    "import powerlaw as pl\n",
    "from scipy.optimize import curve_fit\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "executed-stationery",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simple-booking",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../Data/out.moreno_crime_crime'\n",
    "ROLE = '../Data/rel.moreno_crime_crime.person.role'\n",
    "NAME = '../Data/ent.moreno_crime_crime.person.name'\n",
    "SEX =  '../Data/ent.moreno_crime_crime.person.sex'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mighty-three",
   "metadata": {},
   "source": [
    "# Constants"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cubic-penguin",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacterial-attraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x, a, l):\n",
    "    \"\"\"A custom function which is a power law with its exponential truncation. From 'Atlas for the Aspiring\n",
    "    Network Scientist' Chapter 6\n",
    "    \"\"\"\n",
    "    return (x ** a) * np.exp(-l * x) \n",
    "\n",
    "def log_f(x, a, l):\n",
    "    \"\"\"The Logarithm of f, used to fit the log of the CCDF using curve_fit\"\"\"\n",
    "    return np.log10(f(x, a, l))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "restricted-complex",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worth-helen",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA) as f:\n",
    "    data = f.read().splitlines()\n",
    "    \n",
    "with open(ROLE) as f:\n",
    "    role = f.read().splitlines()\n",
    "    \n",
    "with open(NAME) as f:\n",
    "    name = f.read().splitlines()\n",
    "    \n",
    "with open(SEX) as f:\n",
    "    sex = f.read().splitlines()\n",
    "    \n",
    "for i in range(len(data)):\n",
    "    data[i] = 'p' + data[i] # adds 'p' to the People nodes to differentiate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "third-swaziland",
   "metadata": {},
   "outputs": [],
   "source": [
    "lasts = []\n",
    "firsts = []\n",
    "for n in name:\n",
    "    lasts.append((re.findall('[A-Z][^A-Z]*', n))[0])\n",
    "    firsts.append((re.findall('[A-Z][^A-Z]*', n))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "weighted-portable",
   "metadata": {},
   "outputs": [],
   "source": [
    "B = nx.read_edgelist(data) # read edgelist in\n",
    "print(len(B.nodes()), len(B.edges())) # sanity check (1380, 1476)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pacific-shepherd",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = np.loadtxt(DATA, dtype=str)\n",
    "a = np.unique(graph[:,0]) # people nodes\n",
    "b = np.unique(graph[:,1]) # crime nodes\n",
    "a = ['p' + a for a in a] # add string to differentiate\n",
    "\n",
    "print(f\"There are {len(a)} unique people and {len(b)} unique crime events.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "living-groove",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "forward-crowd",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# add nodes\n",
    "for j in range(len(a)):\n",
    "    #G.add_node(a[j], bipartite=0, data=True, name=name[j], sex=sex[j]) # Add attributes name and sex\n",
    "    G.add_node(a[j], bipartite=0, data=True, name=name[j], first=firsts[j], last=lasts[j], sex=sex[j]) #Add attributes name and sex\n",
    "    \n",
    "G.add_nodes_from(b, bipartite=1, data=True)\n",
    "\n",
    "# add edges\n",
    "i = 0\n",
    "for edge in B.edges():\n",
    "    G.add_edge(edge[0], edge[1], role= [role[i]])\n",
    "    \n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-situation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(\n",
    "    'Num. of nodes: {} \\nNum. of edges: {} \\nIs bipartite? {} \\nIs connected? {}'.format(\n",
    "        G.number_of_nodes(), \n",
    "        G.number_of_edges(), \n",
    "        nx.is_bipartite(G),\n",
    "        nx.is_connected(G)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "critical-competition",
   "metadata": {},
   "source": [
    "From the [official Networkx docs](https://networkx.org/documentation/stable/reference/algorithms/bipartite.html):\n",
    "\n",
    "Many algorithms of the bipartite module of NetworkX require, as an argument, a container with all the nodes that belong to one set, in addition to the bipartite graph B. The functions in the bipartite package do not check that the node set is actually correct nor that the input graph is actually bipartite. If B is connected, you can find the two node sets using a two-coloring algorithm:\n",
    "\n",
    "```py\n",
    ">>> nx.is_connected(B)\n",
    "True\n",
    ">>> bottom_nodes, top_nodes = bipartite.sets(B)\n",
    "```\n",
    "However, if the input graph is not connected, there are more than one possible colorations. This is the reason why we require the user to pass a container with all nodes of one bipartite node set as an argument to most bipartite functions. In the face of ambiguity, we refuse the temptation to guess and raise an AmbiguousSolution Exception if the input graph for bipartite.sets is disconnected.\n",
    "\n",
    "Using the bipartite node attribute, you can easily get the two node sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-fruit",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0} # people\n",
    "bottom_nodes = set(G) - top_nodes      # crimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pleased-discrimination",
   "metadata": {},
   "source": [
    "## Adding Metadata to Nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vulnerable-retention",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.get_node_attributes(G, 'name')['p1']) # check name of person 'p1' = 'AbelDennis'\n",
    "print(nx.get_node_attributes(G, 'sex')['p1']) # check sex of person 'p1' = '1'\n",
    "print(nx.get_edge_attributes(G, 'role')['p1', '1']) # check role of edge between person p1 and crime 1 = 'Suspect'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mature-efficiency",
   "metadata": {},
   "source": [
    "### Add \"Criminal Status\" node attribute to each people node\n",
    "\n",
    "**Here, we are making the assumption that, if a person has been involved in a crime as a \"suspect\" or \"victim suspect\" then they are a criminal. Meanwhile, if someone has only been a \"witness\" or a \"victim\" then they are \"innocent\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "foster-radio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary based on people nodes to keep track of all roles per node\n",
    "p_nodes = {el:[] for el in top_nodes}\n",
    "# Add all the edge attributes to a dictionary of people nodes\n",
    "for key,value in nx.get_edge_attributes(G, 'role').items():\n",
    "    for part in key:\n",
    "        if part in top_nodes:\n",
    "            p_nodes[part].append(value[0])\n",
    "print(p_nodes['p1'])\n",
    "print(p_nodes['p815'])\n",
    "\n",
    "# Initialize a dictionary to keep track of who is a \"criminal\"\n",
    "criminals = {el:[] for el in top_nodes}\n",
    "# Loop through all roles per node, and deem them criminals if ever they have been a suspect\n",
    "for key in p_nodes:\n",
    "    for value in p_nodes[key]:\n",
    "        if value == \"Suspect\":\n",
    "            criminals[key] = \"Criminal\"\n",
    "            break\n",
    "        elif value == \"Victim Suspect\":\n",
    "            criminals[key] = \"Criminal\"\n",
    "            break\n",
    "        else:\n",
    "            criminals[key] = \"Innocent\"\n",
    "print(criminals['p1']) # Should be a Criminal\n",
    "\n",
    "# Convert to pandas df\n",
    "criminals_df = pd.DataFrame(criminals.items(), columns=['node', 'Criminal_Status'])\n",
    "# loop through rows in the data frame and add the attribute of Criminal Status\n",
    "for index, row in criminals_df.iterrows():\n",
    "    #print(row['node'])\n",
    "    G.nodes[row['node']]['Criminal_Status'] = row['Criminal_Status']\n",
    "\n",
    "print(nx.get_node_attributes(G, 'Criminal_Status')['p1']) # check name of person 'p1' = 'Criminal'\n",
    "print(nx.get_node_attributes(G, 'Criminal_Status')['p815']) # check name of person 'p1' = 'Criminal'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conservative-ceiling",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nx.get_edge_attributes(G, 'role')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-rocket",
   "metadata": {},
   "source": [
    "# Finding Largest Connected Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "correct-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components in the graph sorted in descendent order\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "\n",
    "# Selecting the biggest component\n",
    "G0 = G.subgraph(Gcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "treated-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of nodes for the LCC\n",
    "top_nodes_lcc = {n for n, d in G0.nodes(data=True) if d[\"bipartite\"] == 0} # people\n",
    "bottom_nodes_lcc = set(G0) - top_nodes_lcc      # crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demanding-laundry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes for each component and number of components\n",
    "comp_list = []\n",
    "for comp in Gcc:\n",
    "    comp_len = len(comp)\n",
    "    comp_list.append(comp_len)\n",
    "\n",
    "f'Number of nodes for each components {comp_list} and number of components {len(comp_list)}'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-demographic",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "proved-hacker",
   "metadata": {},
   "source": [
    "### Average Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "generic-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Density: {} \\nAverage Clustering: {}'.format(\n",
    "        round(bipartite.density(G, bottom_nodes),5), \n",
    "        round(bipartite.average_clustering(G),5)\n",
    "    )\n",
    ")\n",
    "print('\\n')\n",
    "print('Density LCC: {} \\nAverage Clustering LCC: {}'.format(\n",
    "        round(bipartite.density(G0, bottom_nodes_lcc),5), \n",
    "        round(bipartite.average_clustering(G0),5)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "guided-narrative",
   "metadata": {},
   "source": [
    "### Another way to get Average Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "classical-harris",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = bipartite.clustering(G)\n",
    "print(f'Clustering of Entire Network: {round(sum(cluster.values())/len(cluster.values()),5)}')\n",
    "\n",
    "cluster2 = bipartite.clustering(G0)\n",
    "print(f'Clustering for LCC: {round(sum(cluster2.values())/len(cluster2.values()),5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "labeled-stewart",
   "metadata": {},
   "source": [
    "### Global Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the entire graph\n",
    "global_clustering = bipartite.robins_alexander_clustering(G)\n",
    "print(f'Global Clustering: {round(global_clustering,5)}')\n",
    "\n",
    "# This is for only the LCC\n",
    "global_clustering_lcc = bipartite.robins_alexander_clustering(G0)\n",
    "print(f'Global Clustering LCC: {round(global_clustering_lcc, 5)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hearing-leeds",
   "metadata": {},
   "source": [
    "**EXPLANATION OF CLUSTERING:** TO BE DONE BY SOMEONE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "compliant-pulse",
   "metadata": {},
   "source": [
    "# Nodes stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "descending-orchestra",
   "metadata": {},
   "source": [
    "## Betweenneess Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "widespread-missile",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Whole network\n",
    "betweenness_centrality = bipartite.betweenness_centrality(G, top_nodes)\n",
    "\n",
    "# For the LCC\n",
    "betweenness_centrality_lcc = bipartite.betweenness_centrality(G0, top_nodes_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inner-infection",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1.set_title('Betweenness Centrality of Whole Network')\n",
    "ax2.set_title('Betweenness Centrality of LCC')\n",
    "ax1.hist(betweenness_centrality.values(), bins=10, log=True)\n",
    "ax2.hist(betweenness_centrality_lcc.values(), bins=10, log=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "miniature-worker",
   "metadata": {},
   "source": [
    "## Closeness Centrality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "religious-shape",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Whole Network\n",
    "closeness_centrality = bipartite.closeness_centrality(G, top_nodes)\n",
    "\n",
    "# For the LCC\n",
    "closeness_centrality_lcc = bipartite.closeness_centrality(G0, top_nodes_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-forest",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1.set_title('Closeness Centrality of Whole Network')\n",
    "ax2.set_title('Closeness Centrality of LCC')\n",
    "ax1.hist(closeness_centrality.values(), bins=75)\n",
    "ax2.hist(closeness_centrality_lcc.values(), bins=75)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nervous-manufacturer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# People node with the most neighbors (Max left degree)\n",
    "node_neighbors = {n: len(set(G.neighbors(n))) for n in top_nodes}\n",
    "maxnode = max(node_neighbors, key = node_neighbors.get)\n",
    "print(maxnode, node_neighbors[maxnode])\n",
    "\n",
    "# People node in LCC with most neighbors\n",
    "node_neighbors_lcc = {n: len(set(G0.neighbors(n))) for n in top_nodes_lcc}\n",
    "maxnode_lcc = max(node_neighbors_lcc, key = node_neighbors_lcc.get)\n",
    "print(maxnode_lcc, node_neighbors_lcc[maxnode_lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-palace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime node with the most neighbors (Max right degree)\n",
    "node_neighbors = {n: len(set(G.neighbors(n))) for n in bottom_nodes}\n",
    "maxnode = max(node_neighbors, key = node_neighbors.get)\n",
    "print(maxnode, node_neighbors[maxnode])\n",
    "\n",
    "node_neighbors_lcc = {n: len(set(G0.neighbors(n))) for n in bottom_nodes_lcc}\n",
    "maxnode_lcc = max(node_neighbors_lcc, key = node_neighbors_lcc.get)\n",
    "print(maxnode_lcc, node_neighbors_lcc[maxnode_lcc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-record",
   "metadata": {},
   "source": [
    "**ADD EXPLANATION**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "vital-there",
   "metadata": {},
   "source": [
    "## Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "present-authority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Distribution of the Entire Network, Combining People and Crime Nodes\n",
    "dd = Counter(dict(G.degree).values())\n",
    "dd = pd.DataFrame(list(dd.items()), columns = (\"Degree (d)\", \"Frequency\")).sort_values(by = \"Degree (d)\")\n",
    "\n",
    "# Degree Distribution of the LCC, Combining People and Crime Nodes\n",
    "dd_lcc = Counter(dict(G0.degree).values())\n",
    "dd_lcc = pd.DataFrame(list(dd_lcc.items()), columns = (\"Degree (d)\", \"Frequency\")).sort_values(by = \"Degree (d)\")\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(12,10)) \n",
    "dd.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", ax=axs[0,0], color = \"#e41a1c\", title=\"Degree Distribution of All Nodes\")\n",
    "dd.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", ax=axs[0,1], color = \"#e41a1c\", loglog = True, title=\"Loglog Degree Distribution of All Nodes\")\n",
    "\n",
    "dd_lcc.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", ax=axs[1,0], color = \"#e41a1c\", title=\"Degree Distribution of LCC\")\n",
    "dd_lcc.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", ax=axs[1,1], color = \"#e41a1c\", loglog = True, title=\"Loglog Degree Distribution of LCC\")\n",
    "#plt.savefig(\"../Report/degree_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "difficult-planning",
   "metadata": {},
   "source": [
    "## People VS Crime Node Degree Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "infinite-sunset",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes the entire graph\n",
    "e_p = []\n",
    "e_c = []\n",
    "for k in G.nodes(): # Loop through all nodes\n",
    "    if G.nodes[k][\"bipartite\"] == 0: # If People Nodes\n",
    "        e_p.append(len(G.edges(k)))  # Append number of edges to each node\n",
    "    else:                            # If Crime Nodes\n",
    "        e_c.append(len(G.edges(k)))  # Append number of edges to each crime\n",
    "\n",
    "e_cc = Counter(e_c)\n",
    "e_pp = Counter(e_p)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(12,10))\n",
    "fig.suptitle('Degree Distributions of the Network')\n",
    "\n",
    "ax1.scatter(e_cc.keys(), e_cc.values())\n",
    "ax1.set_title('Crime Nodes')\n",
    "ax1.set_xlabel('Degree')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.scatter(e_pp.keys(), e_pp.values())\n",
    "ax2.set_title('People Nodes')\n",
    "ax2.set_xlabel('Degree')\n",
    "ax2.set_ylabel('Count');\n",
    "\n",
    "# This only deals with the LCC\n",
    "# Lists of number of edges per node\n",
    "e_p1 = [] # Number of edges connected to each People node\n",
    "e_c1 = [] # Number of edges connected to each Crime node\n",
    "for k in G0.nodes(): # loop through all nodes\n",
    "    if G0.nodes[k][\"bipartite\"] == 0:  # If People Node\n",
    "        e_p1.append(len(G0.edges(k)))  # Append number of edges to each node\n",
    "    else:                              # If Crime Node\n",
    "        e_c1.append(len(G0.edges(k)))  # Append number of edges to each node\n",
    "\n",
    "e_cc1 = Counter(e_c1)\n",
    "e_pp1 = Counter(e_p1)\n",
    "\n",
    "#fig, (ax1, ax2) = plt.subplots(1,2, figsize=(12,4))\n",
    "#fig.suptitle('Degree Distributions of the Largest Connected Component')\n",
    "\n",
    "ax3.scatter(e_cc1.keys(), e_cc1.values())\n",
    "ax3.set_title('Crime Nodes of LCC')\n",
    "ax3.set_xlabel('Degree')\n",
    "ax3.set_ylabel('Count')\n",
    "\n",
    "ax4.scatter(e_pp1.keys(), e_pp1.values())\n",
    "ax4.set_title('People Nodes of LCC')\n",
    "ax4.set_xlabel('Degree')\n",
    "ax4.set_ylabel('Count');\n",
    "len(e_p), len(e_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adult-aquarium",
   "metadata": {},
   "source": [
    "## CCDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "favorite-hamilton",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Degree distribution\n",
    "ccdf = dd.sort_values(by = \"Degree (d)\", ascending = False)\n",
    "ccdf[\"cumsum\"] = ccdf[\"Frequency\"].cumsum()\n",
    "ccdf[\"ccdf: P(X>=d)\"] = ccdf[\"cumsum\"] / ccdf[\"Frequency\"].sum()\n",
    "ccdf = ccdf[[\"Degree (d)\", \"ccdf: P(X>=d)\"]].sort_values(by = \"Degree (d)\")\n",
    "\n",
    "ccdf.plot(kind = \"line\", x = \"Degree (d)\", y = \"ccdf: P(X>=d)\", color = \"#e41a1c\", loglog = True)\n",
    "#plt.savefig(\"../Report/degree_distribution_ccdf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "level-venice",
   "metadata": {},
   "source": [
    "## Fit CCDF to PowerLaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southwest-regulation",
   "metadata": {},
   "outputs": [],
   "source": [
    "logcdf = np.log10(ccdf[[\"Degree (d)\", \"ccdf: P(X>=d)\"]])\n",
    "slope, log10intercept, r_value, p_value, std_err = linregress(logcdf[\"Degree (d)\"], logcdf[\"ccdf: P(X>=d)\"])\n",
    "print(\"CCDF Fit: %1.4f x ^ %1.4f (R2 = %1.4f, p = %1.4f)\" % (10 ** log10intercept, slope, r_value ** 2, p_value))\n",
    "print(\"\\n\")\n",
    "\n",
    "results = pl.Fit(ccdf[\"ccdf: P(X>=d)\"])\n",
    "k_min = ccdf[ccdf[\"ccdf: P(X>=d)\"] == results.power_law.xmin][\"Degree (d)\"]\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-ballot",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best fit.\n",
    "ccdf[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf[\"Degree (d)\"] ** -results.power_law.alpha)\n",
    "ax = plt.gca()\n",
    "ccdf.plot(kind = \"line\", x = \"Degree (d)\", y = \"ccdf: P(X>=d)\", color = \"#e41a1c\", loglog = True, ax = ax)\n",
    "ccdf.plot(kind = \"line\", x = \"Degree (d)\", y = \"fit\", color = \"#377eb8\", loglog = True, ax = ax)\n",
    "#plt.savefig(\"../Report/ccdf_fit.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sustainable-headquarters",
   "metadata": {},
   "source": [
    "## Degree Distributions Per Node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-enzyme",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes the entire network\n",
    "\n",
    "# Dictionary of Crime Nodes and Their Degree\n",
    "crimes_degree = list(G.degree(b))\n",
    "crimes_degree = pd.DataFrame(crimes_degree, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Dictionary of Crime Node Degrees and Their Frequencies\n",
    "crimes_degree_count = Counter(dict(G.degree(b)).values())\n",
    "crimes_degree_count = pd.DataFrame(list(crimes_degree_count.items()), columns = (\"Degree (d)\", \"Frequency\")).sort_values(by = \"Degree (d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "revolutionary-bankruptcy",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# This includes the entire network\n",
    "\n",
    "# Dictionary of People Nodes and Their Degree\n",
    "ppl_degree = list(G.degree(a))\n",
    "ppl_degree = pd.DataFrame(ppl_degree, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Dictionary of People Node Degrees and Their Frequencies\n",
    "ppl_degree_count = Counter(dict(G.degree(a)).values())\n",
    "ppl_degree_count = pd.DataFrame(list(ppl_degree_count.items()), columns = (\"Degree (d)\", \"Frequency\")).sort_values(by = \"Degree (d)\")\n",
    "#ppl_degree_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-billy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes the entire network\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "crimes_degree_count.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", color = \"#fb00ff\", ax = ax, alpha=0.9, s=200)\n",
    "ppl_degree_count.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", color = \"#ff9d00\", ax = ax, alpha=0.7, s=200)\n",
    "ax.legend([\"Crimes\",\"People\"])\n",
    "plt.xticks(np.arange(0, 26, step=1))\n",
    "plt.title(\"Degree Distributions per Node Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "external-processor",
   "metadata": {},
   "source": [
    "## Projected Plots of the Entire Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opening-measure",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Entire Network of People\n",
    "P_w = bipartite.weighted_projected_graph(G, top_nodes)\n",
    "nx.draw(P_w, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "detailed-allergy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Entire Network of Crimes\n",
    "C_w = bipartite.weighted_projected_graph(G, bottom_nodes)\n",
    "nx.draw(C_w, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collect-atlanta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot only biggest component\n",
    "pos = nx.spring_layout(G0)\n",
    "posB = nx.bipartite_layout(G0, top_nodes)\n",
    "biggest_comp_graph = nx.draw(G0, pos, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interior-shannon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biggest component bipartite plot\n",
    "biggest_comp_graph = nx.draw(G0, posB, node_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "central-joint",
   "metadata": {},
   "source": [
    "## Projected Graphs from only the LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equal-search",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the People from the LCC\n",
    "P = bipartite.weighted_projected_graph(G0, top_nodes_lcc)\n",
    "nx.draw(P, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "received-addition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Crimes from the LCC\n",
    "C = bipartite.weighted_projected_graph(G0, bottom_nodes_lcc)\n",
    "nx.draw(C, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "southeast-measurement",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-athens",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraph of Criminals from LCC\n",
    "# selected just the suspect edges\n",
    "criminal_edges = [(p,c) for p,c,e in G0.edges(data=True) if nx.get_node_attributes(G0, 'Criminal_Status')[p] == 'Criminal']\n",
    "# getting all nodes that have suspect edges\n",
    "sus_nodes = pd.DataFrame(suspect_edges, columns=['node', 'crime']).iloc[:,0].unique()\n",
    "sus_nodes = set(sus_nodes)\n",
    "\n",
    "# Create the Graph\n",
    "GCriminal = G0.edge_subgraph(criminal_edges)\n",
    "\n",
    "# Visualize The Bipartite Criminal Subgraphs\n",
    "pos = nx.spring_layout(GCriminal, seed=42)\n",
    "nx.draw(GCriminal, pos, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "opened-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Criminal Subgraph\n",
    "# Get people and crime nodes\n",
    "people_nodes_GS = {n for n, d in GCriminal.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "crime_nodes_GS = set(GCriminal) - people_nodes_GS\n",
    "\n",
    "# Create a new projected graph\n",
    "weighted_criminal_graph = nx.bipartite.weighted_projected_graph(GCriminal, people_nodes_GS, ratio=False)\n",
    "\n",
    "# give the number of shared neighbors\n",
    "weights = list(nx.get_edge_attributes(weighted_criminal_graph, 'weight').values())\n",
    "\n",
    "nx.draw_spring(weighted_criminal_graph,node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stunning-contact",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subgraph of Innocents from LCC\n",
    "# selected just the Innocent edges\n",
    "innocent_edges = [(p,c) for p,c,e in G0.edges(data=True) if nx.get_node_attributes(G0, 'Criminal_Status')[p] == 'Innocent']\n",
    "# getting all nodes that have suspect edges\n",
    "inno_nodes = pd.DataFrame(innocent_edges, columns=['node', 'crime']).iloc[:,0].unique()\n",
    "inno_nodes = set(sus_nodes)\n",
    "\n",
    "# Create the Graph\n",
    "GInnocent = G0.edge_subgraph(innocent_edges)\n",
    "\n",
    "# Visualize The Bipartite Criminal Subgraphs\n",
    "pos = nx.spring_layout(GInnocent, seed=42)\n",
    "nx.draw(GInnocent, pos, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sacred-quality",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project Innocent Subgraph from LCC\n",
    "# Get people and crime nodes\n",
    "people_nodes_GS = {n for n, d in GInnocent.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "crime_nodes_GS = set(GInnocent) - people_nodes_GS\n",
    "\n",
    "# Create a new projected graph\n",
    "weighted_innocent_graph = nx.bipartite.weighted_projected_graph(GInnocent, people_nodes_GS, ratio=False)\n",
    "\n",
    "# give the number of shared neighbors\n",
    "weights = list(nx.get_edge_attributes(weighted_innocent_graph, 'weight').values())\n",
    "\n",
    "nx.draw_spring(weighted_innocent_graph, node_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-illness",
   "metadata": {},
   "source": [
    "# Compare Weight Methods\n",
    "\n",
    "Based on exercise 23.2, and P. 324 in Network Science"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make the code from the previous question into its own function\n",
    "def simple(network, nodes):\n",
    "    T = nx.algorithms.bipartite.matrix.biadjacency_matrix(network, row_order = nodes)\n",
    "    U = T * T.T\n",
    "    U.setdiag(0)\n",
    "    U.eliminate_zeros()\n",
    "    G = nx.from_scipy_sparse_matrix(U)\n",
    "    return nx.relabel_nodes(G, {i: nodes[i] for i in range(len(nodes))}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subtle-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make functions to calculate alternative projections. The scheme is the same:\n",
    "# calculate pairwise distances from the bipartite adjacency matrix and make them\n",
    "# into similarity measures.\n",
    "def cosine(network, nodes):\n",
    "    T = nx.algorithms.bipartite.matrix.biadjacency_matrix(network, row_order = nodes)\n",
    "    # Since this is a distance, we take the inverse to make it into a similarity.\n",
    "    # Maximum possible cosine distance is 1, so we take one minus it. If the two nodes\n",
    "    # were identical, this measure equals to 1, if they were the maximum posible far\n",
    "    # apart, this measure is zero.\n",
    "    j_dist = 1.0 - pairwise_distances(T, metric = \"cosine\", n_jobs = -1)\n",
    "    np.fill_diagonal(j_dist, 0)\n",
    "    G = nx.from_numpy_matrix(j_dist)\n",
    "    return nx.relabel_nodes(G, {i: nodes[i] for i in range(len(nodes))})\n",
    "\n",
    "def pearson(network, nodes):\n",
    "    T = nx.algorithms.bipartite.matrix.biadjacency_matrix(network, row_order = nodes)\n",
    "    # Correlation distance maximum is 2 (corresponding to pearson correlation of -1).\n",
    "    j_dist = (2.0 - pairwise_distances(T.todense(), metric = \"correlation\", n_jobs = -1)) / 2\n",
    "    np.fill_diagonal(j_dist, 0)\n",
    "    # We remove all edges between nodes negatively correlated with each other.\n",
    "    j_dist[j_dist < .5] = 0\n",
    "    G = nx.from_numpy_matrix(j_dist)\n",
    "    return nx.relabel_nodes(G, {i: nodes[i] for i in range(len(nodes))})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-order",
   "metadata": {},
   "outputs": [],
   "source": [
    "ppl = list(top_nodes)\n",
    "crimes = list(bottom_nodes)\n",
    "nodes = ppl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limiting-extreme",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "\n",
    "G_simple = simple(G, nodes)\n",
    "G_cosine = cosine(G, nodes)\n",
    "G_pearson = pearson(G, nodes)\n",
    "all_edges = set(G_simple.edges) | set(G_cosine.edges) | set(G_pearson.edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "miniature-battle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Pearson Correlation of the Cosine and Pearson weights as compared to the simple weights\n",
    "df = pd.DataFrame(data = [(e,\n",
    "                           G_simple.edges[e][\"weight\"] if e in G_simple.edges else 0, \n",
    "                           G_cosine.edges[e][\"weight\"] if e in G_cosine.edges else 0, \n",
    "                           G_pearson.edges[e][\"weight\"] if e in G_pearson.edges else 0)\n",
    "                         for e in all_edges], columns = (\"edge\", \"simple\", \"cosine\", \"pearson\"))\n",
    "\n",
    "df.set_index(\"edge\").corr()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "connected-security",
   "metadata": {},
   "source": [
    "The correlation coefficient of -0.08, in comparing both the cosine similarity and Pearson correlation indicates an insignificant level of negatively correlated. Meaning that there is no statistically significant reason for choosing cosine or pearson correlation for edge weighting above simple edge weighting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retained-feature",
   "metadata": {},
   "source": [
    "## Degree Distribution Using only the LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-tuition",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of People Nodes and their Degree\n",
    "p_deg_lcc = list(G0.degree(a))\n",
    "p_deg_lcc = pd.DataFrame(p_deg_lcc, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# DataFrame of People Node Degrees and their Frequency\n",
    "p_deg_lcc_count = Counter(dict(G0.degree(a)).values())\n",
    "p_deg_lcc_count = pd.DataFrame(list(p_deg_lcc_count.items()), columns = (\"Degree (d)\", \"Frequency\")).sort_values(by = \"Degree (d)\")\n",
    "#p_deg_lcc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "illegal-ladder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of Crime Nodes and their Degree\n",
    "c_deg_lcc = list(G0.degree(b))\n",
    "c_deg_lcc = pd.DataFrame(c_deg_lcc, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Create DataFrame of Crime Node Degrees and their Frequency\n",
    "c_deg_lcc_count = Counter(dict(G0.degree(b)).values())\n",
    "c_deg_lcc_count = pd.DataFrame(list(c_deg_lcc_count.items()), columns = (\"Degree (d)\", \"Frequency\")).sort_values(by = \"Degree (d)\")\n",
    "#crimes_degree_lcc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "guided-management",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Uses Only the LCC\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax = plt.gca()\n",
    "c_deg_lcc_count.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", color = \"#fb00ff\", ax = ax, alpha=0.9, s=150)\n",
    "p_deg_lcc_count.plot(kind = \"scatter\", x = \"Degree (d)\", y = \"Frequency\", color = \"#ff9d00\", ax = ax, alpha=0.7, s=150)\n",
    "\n",
    "ax.legend([\"Crimes\",\"People\"])\n",
    "plt.xticks(np.arange(0, 26, step=1))\n",
    "plt.title(\"Degree Distributions per Node Category Using Only LCC\")\n",
    "#plt.savefig(\"../Report/degree_distribution_lcc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "metric-phase",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Uses Only the LCC\n",
    "fig, ax = plt.subplots(figsize=(8,5)) \n",
    "ax = plt.gca()\n",
    "c_deg_lcc_count.plot(kind = \"scatter\", loglog=True, x = \"Degree (d)\", y = \"Frequency\", color = \"#fb00ff\", ax = ax, alpha=0.9, s=150)\n",
    "p_deg_lcc_count.plot(kind = \"scatter\", loglog=True, x = \"Degree (d)\", y = \"Frequency\", color = \"#ff9d00\", ax = ax, alpha=0.7, s=150)\n",
    "\n",
    "ax.legend([\"Crimes\",\"People\"])\n",
    "plt.title(\"Degree Distributions per Node Category Using Only LCC\")\n",
    "#plt.savefig(\"../Report/degree_distribution_loglog_lcc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "responsible-chance",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution of Both Nodes in LCC\n",
    "ccdf_lcc = dd_lcc.sort_values(by = \"Degree (d)\", ascending = False)\n",
    "ccdf_lcc[\"cumsum\"] = ccdf_lcc[\"Frequency\"].cumsum()\n",
    "ccdf_lcc[\"ccdf: P(X>=d)\"] = ccdf_lcc[\"cumsum\"] / ccdf_lcc[\"Frequency\"].sum()\n",
    "ccdf_lcc = ccdf_lcc[[\"Degree (d)\", \"ccdf: P(X>=d)\"]].sort_values(by = \"Degree (d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "competent-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime Degree Distribution Using Only LCC\n",
    "c_ccdf = c_deg_lcc_count.sort_values(by = \"Degree (d)\", ascending = False)\n",
    "c_ccdf[\"cumsum\"] = c_ccdf[\"Frequency\"].cumsum()\n",
    "c_ccdf[\"ccdf: P(X>=d)\"] = c_ccdf[\"cumsum\"] / c_ccdf[\"Frequency\"].sum()\n",
    "c_ccdf = c_ccdf[[\"Degree (d)\", \"ccdf: P(X>=d)\"]].sort_values(by = \"Degree (d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "noticed-applicant",
   "metadata": {},
   "outputs": [],
   "source": [
    "# People Degree Distribution Using Only LCC\n",
    "p_ccdf = p_deg_lcc_count.sort_values(by = \"Degree (d)\", ascending = False)\n",
    "p_ccdf[\"cumsum\"] = p_ccdf[\"Frequency\"].cumsum()\n",
    "p_ccdf[\"ccdf: P(X>=d)\"] = p_ccdf[\"cumsum\"] / p_ccdf[\"Frequency\"].sum()\n",
    "p_ccdf = p_ccdf[[\"Degree (d)\", \"ccdf: P(X>=d)\"]].sort_values(by = \"Degree (d)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fourth-geology",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,4)) \n",
    "ax = plt.gca()\n",
    "\n",
    "c_ccdf.plot(kind = \"line\", x = \"Degree (d)\", y = \"ccdf: P(X>=d)\", ax=axs[0], color = \"#fb00ff\", loglog = True)\n",
    "p_ccdf.plot(kind = \"line\", x = \"Degree (d)\", y = \"ccdf: P(X>=d)\", ax=axs[0], color = \"#ff9d00\", loglog = True)\n",
    "axs[0].legend([\"Crimes\",\"People\"])\n",
    "axs[0].set_title(\"CCDF per Node Category Using Only LCC\")\n",
    "\n",
    "ccdf_lcc.plot(kind = \"line\", x = \"Degree (d)\", y = \"ccdf: P(X>=d)\", ax=axs[1], color = \"r\", loglog = True)\n",
    "axs[1].legend([\"Both Nodes Combined\"])\n",
    "axs[1].set_title(\"CCDF For All Nodes in LCC\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resident-cheat",
   "metadata": {},
   "source": [
    "## Fit CCDF to PowerLaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "clean-number",
   "metadata": {},
   "outputs": [],
   "source": [
    "logcdf_lcc = np.log10(ccdf_lcc[[\"Degree (d)\", \"ccdf: P(X>=d)\"]])\n",
    "slope, log10intercept, r_value, p_value, std_err = linregress(logcdf_lcc[\"Degree (d)\"], logcdf_lcc[\"ccdf: P(X>=d)\"])\n",
    "print(\"CCDF Fit: %1.4f x ^ %1.4f (R2 = %1.4f, p = %1.4f)\" % (10 ** log10intercept, slope, r_value ** 2, p_value))\n",
    "print(\"\\n\")\n",
    "\n",
    "results = pl.Fit(ccdf_lcc[\"ccdf: P(X>=d)\"])\n",
    "k_min = ccdf_lcc[ccdf_lcc[\"ccdf: P(X>=d)\"] == results.power_law.xmin][\"Degree (d)\"]\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "superb-settle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best fit.\n",
    "ccdf_lcc[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf_lcc[\"Degree (d)\"] ** -results.power_law.alpha)\n",
    "ax = plt.gca()\n",
    "ccdf_lcc.plot(kind = \"line\", x = \"Degree (d)\", y = \"ccdf: P(X>=d)\", color = \"#e41a1c\", loglog = True, ax = ax)\n",
    "ccdf_lcc.plot(kind = \"line\", x = \"Degree (d)\", y = \"fit\", color = \"#377eb8\", loglog = True, ax = ax)\n",
    "#plt.savefig(\"../Report/ccdf_lcc_fit.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "confirmed-center",
   "metadata": {},
   "source": [
    "# Adjacency Matrix\n",
    "\n",
    "Central Place Theory.\n",
    "Chapter 29 in the book.\n",
    "\n",
    "The Adjacency matrix of our LCC looks like a discrete model. Which imposes a strict core-periphery structure. This also does not seem to show any smaller communities extending off of the core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "light-start",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the degrees of the nodes\n",
    "degree = dict(G0.degree)\n",
    "\n",
    "# Figure out which node is of which type\n",
    "rows, cols = nx.algorithms.bipartite.sets(G0)\n",
    "\n",
    "# Sort rows and columns according to their degree\n",
    "rows = {n: degree[n] for n in rows}\n",
    "cols = {n: degree[n] for n in cols}\n",
    "rows = [n[0] for n in sorted(rows.items(), key = lambda x : x[1], reverse = True)]\n",
    "cols = [n[0] for n in sorted(cols.items(), key = lambda x : x[1], reverse = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worldwide-lodge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the biadjacency matrix with the properly sorted rows and columns\n",
    "B = nx.algorithms.bipartite.matrix.biadjacency_matrix(G0, rows, column_order = cols)\n",
    "plt.figure(figsize=(4,10))\n",
    "plt.spy(B, markersize = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "medieval-pennsylvania",
   "metadata": {},
   "source": [
    "# Finding Which Nodes Connect to Which Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "useful-partition",
   "metadata": {},
   "source": [
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.core.core_number.html#networkx.algorithms.core.core_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-process",
   "metadata": {},
   "outputs": [],
   "source": [
    "c_whole = nx.algorithms.core.core_number(G)\n",
    "c_whole_count = Counter(c_whole.values())\n",
    "c_whole_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eastern-links",
   "metadata": {},
   "outputs": [],
   "source": [
    "c = nx.algorithms.core.core_number(G0)\n",
    "count_c = Counter(c.values())\n",
    "count_c\n",
    "print(f\"Core 1 has {count_c[1]} Nodes attached \\nCore 2 has {count_c[2]} Nodes \\nCore 3 has {count_c[3]} Nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "small-license",
   "metadata": {},
   "source": [
    "# Edge-Centric Plot\n",
    "\n",
    "Based on Exercise 27.1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "familiar-seventh",
   "metadata": {},
   "source": [
    "https://www.networkatlas.eu/exercise.htm?c=27&e=1\n",
    "\n",
    "look for how to put a colorbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "changing-effect",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the degrees of the nodes\n",
    "degrees = dict(G0.degree)\n",
    "\n",
    "# Edge-centric plot\n",
    "edge_assort = [(degrees[e[0]], degrees[e[1]]) for e in G0.edges]\n",
    "edge_assort = pd.DataFrame(data = edge_assort, columns = (\"k1\", \"k2\"))\n",
    "edge_assort = edge_assort.groupby(by = [\"k1\", \"k2\"]).size().reset_index().rename(columns = {0: \"count\"}).sort_values(by = \"count\")\n",
    "\n",
    "# Taking the log count of edges with nodes with the same degrees\n",
    "edge_assort[\"count\"] = np.log(edge_assort[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "crude-selection",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the edge-centric plot\n",
    "ax = plt.gca()\n",
    "plt.scatter(edge_assort[\"k1\"], edge_assort[\"k2\"], c = edge_assort[\"count\"], cmap = \"Reds\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "embedded-elimination",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make a new figure\n",
    "plt.clf()\n",
    "\n",
    "# Two vectors with the degree of a node in one, and the average neighbor degree in the other\n",
    "node_assort = [(degrees[n], degrees[n2]) for n in G0.nodes for n2 in G0.neighbors(n)]\n",
    "node_assort = pd.DataFrame(data = node_assort, columns = (\"k\", \"neigh_k\"))\n",
    "node_assort = node_assort.groupby(by = \"k\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "economic-majority",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the node-centric plot\n",
    "ax = plt.gca()\n",
    "plt.scatter(node_assort[\"k\"], node_assort[\"neigh_k\"])\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "anonymous-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's make two vectors with the degrees at each endpoints of edges\n",
    "x1 = []\n",
    "y1 = []\n",
    "for e in G0.edges:\n",
    "    x1.append(degrees[e[0]])\n",
    "    y1.append(degrees[e[1]])\n",
    "    x1.append(degrees[e[1]])\n",
    "    y1.append(degrees[e[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wound-rwanda",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import pearsonr, linregress\n",
    "\n",
    "# Edge-centric assortativity is the pearson correlation of such vectors\n",
    "r, p = pearsonr(x1, y1)\n",
    "print(\"Degree assortativity coefficient: %1.4f (p-value < 0.001? %s)\" % (r, p < 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "public-heritage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-centric assortativity is the power-fit of such vectors\n",
    "slope, intercept, r_value, p_value, std_err = linregress(np.log(node_assort[\"k\"]), np.log(node_assort[\"neigh_k\"]))\n",
    "print(\"Degree assortativity coefficient: %1.4f (p-value < 0.001? %s)\" % (slope, p_value < 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "perfect-teddy",
   "metadata": {},
   "source": [
    "# Configuration Model\n",
    "\n",
    "P. 323 in Network Science:\n",
    "One can define a series of null bipartite network models... via ... a configuration model. These null models will give birth to a bunch of null projections, which will give an expected weight for all possible edges in the unipartite network. Then, you can keep in your projection only those links significantly exceeding random expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-specialist",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare to a null model based on the LCC, unipartite\n",
    "G0degrees = list(dict(G0.degree).values())\n",
    "CM = nx.configuration_model(G0degrees)\n",
    "#nx.draw(CM, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "medical-spine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bipartite configuration model based on the entire dataset\n",
    "BiCM = nx.algorithms.bipartite.generators.configuration_model(e_p, e_c) # imputs are the degrees of each node\n",
    "#nx.draw(Bi_CM, node_size=10)\n",
    "\n",
    "# Find the LCC\n",
    "# Sort the connected components\n",
    "cc_BiCM = sorted(nx.connected_components(BiCM), key=len, reverse=True)\n",
    "\n",
    "# Select the biggest component\n",
    "GCC_BiCM = BiCM.subgraph(cc_BiCM[0])\n",
    "\n",
    "#nx.draw(G0_BiCM, node_size=10)\n",
    "\n",
    "# Create lists of nodes for the LCC\n",
    "people_gcc_BiCM = {n for n, d in GCC_BiCM.nodes(data=True) if d[\"bipartite\"] == 0} # people\n",
    "crimes_gcc_BiCM = set(GCC_BiCM) - people_gcc_BiCM  # crimes\n",
    "\n",
    "print(f\"The BiCM is bipartite? {bipartite.is_bipartite(BiCM)}\")\n",
    "print(\"WHY THE FUCK WON'T THIS WORK?!!!!\")\n",
    "\n",
    "#people_gcc_BiCM\n",
    "\n",
    "# Project This onto the People Nodes... how do do this?\n",
    "P_BiCM = bipartite.weighted_projected_graph(GCC_BiCM, people_gcc_BiCM)\n",
    "#nx.algorithms.bipartite.projection.projected_graph(GCC_BiCM, people_gcc_BiCM, multigraph=True)\n",
    "#plt.rcParams.update({'figure.figsize': (20, 10)})\n",
    "#pos = nx.spring_layout(P_BiCM, seed=42)\n",
    "#nx.draw_networkx(P_BiCM, pos, node_size=20, with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "honest-genre",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laden-electricity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bipartite that is only the LCC\n",
    "Bi_CM_0 = nx.algorithms.bipartite.generators.configuration_model(e_p1, e_c1)\n",
    "#nx.draw(Bi_CM_0, node_size=10)\n",
    "\n",
    "# Project the BiCM generated network\n",
    "#Proj_BiCM = bipartite.weighted_projected_graph(Bi_CM_0, e_p1)\n",
    "#nx.draw(Proj_BiCM, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "falling-runner",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration model based on the projection of people\n",
    "P_nodes_projected = list(dict(P.degree).values())\n",
    "P_CM = nx.configuration_model(P_nodes_projected)\n",
    "nx.draw(P_CM, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-valve",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "about-recorder",
   "metadata": {},
   "source": [
    "# Checking for Assortativity (Homophily) of Gender\n",
    "\n",
    "### *Assortativity measures the similarity of connections in the graph with respect to the given attribute. Perfect assortativity is 1, while perfect disassortativity is -1. The whole network is quite disassortative.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smart-portrait",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativity coefficient of the Entire Network\n",
    "nx.attribute_assortativity_coefficient(G, \"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativity coefficient of the LCC\n",
    "nx.attribute_assortativity_coefficient(G0, \"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "married-current",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativity coefficient of the projected network\n",
    "nx.attribute_assortativity_coefficient(P, \"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "answering-outline",
   "metadata": {},
   "source": [
    "## Checking Assortativity of Criminal_status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-sweet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativity coefficient of the projected people network (based on the LCC of the entire network)\n",
    "nx.attribute_assortativity_coefficient(P, \"Criminal_Status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-gazette",
   "metadata": {},
   "source": [
    "#### More powerlaw stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convertible-holocaust",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Use the power law (f function) with its logarithm (log_f), to fit it to the log of the CCDF in curve_fit. This\n",
    "# is done because we want to minimize the relative error, not the absolute error (since the tail of the \n",
    "# distribution is very important, but it contributes very little to the absolute error).\n",
    "\n",
    "#popt, pcov = curve_fit(log_f, ccdf_lcc[\"Degree (d)\"], np.log10(ccdf_lcc[\"ccdf: P(X>=d)\"]), p0 = (1, 1))\n",
    "#ccdf_lcc[\"fit\"] = ccdf_lcc.apply(lambda x: f(x[\"Degree (d)\"], popt[0], popt[1]), axis = 1)\n",
    "\n",
    "#ax = plt.gca()\n",
    "#ccdf_lcc.plot(kind = \"line\", x = \"Degree (d)\", y = \"ccdf: P(X>=d)\", color = \"#e41a1c\", loglog = True, ax = ax)\n",
    "#ccdf_lcc.plot(kind = \"line\", x = \"Degree (d)\", y = \"fit\", color = \"#377eb8\", loglog = True, ax = ax)\n",
    "#plt.savefig(\"../Report/ccdf_lcc_fit_2.png\")\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accompanied-wrist",
   "metadata": {},
   "source": [
    "### Non-square adjacency matrix, and stocastic matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "color-board",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-square adjacency matrix\n",
    "adjmat = nx.algorithms.bipartite.matrix.biadjacency_matrix(G, top_nodes)\n",
    "print(adjmat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "least-championship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project along smaller axis\n",
    "if adjmat.shape[0] == 551:\n",
    "    adjmat_proj = adjmat.dot(adjmat.T)\n",
    "else:\n",
    "    adjmat_proj = adjmat.T.dot(adjmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "encouraging-dryer",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make stocastic\n",
    "adjmat_proj_stoc = adjmat_proj / adjmat_proj.sum(axis = 1)\n",
    "print(adjmat_proj_stoc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "little-disabled",
   "metadata": {},
   "source": [
    "# Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educated-textbook",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.lines as mlines\n",
    "import networkx.algorithms.community as comms\n",
    "class Dunno:\n",
    "    \n",
    "    def __init__ (self, G, POI='p1', highlight=0, Gcc=True):\n",
    "        \n",
    "        if Gcc:\n",
    "            comp_list = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "            self.G = G.subgraph(comp_list[0])\n",
    "        else:\n",
    "            self.G = G\n",
    "        self.POI = POI\n",
    "        self.highlight = highlight\n",
    "        self.communities = sorted(comms.greedy_modularity_communities(self.G), key=len, reverse=True)\n",
    "        self.modularity = comms.modularity(self.G, self.communities)\n",
    "        self.p_nodes = []\n",
    "        self.c_nodes = []\n",
    "        for node in self.G.nodes:\n",
    "            if 'p' in node:\n",
    "                self.p_nodes.append(node)\n",
    "            else:\n",
    "                self.c_nodes.append(node)\n",
    "                \n",
    "        if len(self.c_nodes) == 0:\n",
    "            self.proj = self.G\n",
    "            self.isproj=True\n",
    "            print('This graph only has people in it, no crimes')\n",
    "        else:\n",
    "            self.proj = bipartite.weighted_projected_graph(self.G, self.p_nodes)\n",
    "            self.isproj=False\n",
    "        \n",
    "    def com_find(self, proj=False):\n",
    "        if proj:\n",
    "            self.communities = sorted(comms.greedy_modularity_communities(self.proj), key=len, reverse=True)\n",
    "            self.modularity = comms.modularity(self.proj, self.communities)\n",
    "            \n",
    "        print(f\"There are {len(self.communities)} communities.\")\n",
    "        found = False\n",
    "        for i in range(len(self.communities)):\n",
    "            if self.POI in self.communities[i]:\n",
    "                print(f'The POI is in community {i}')\n",
    "                found = True\n",
    "                self.highlight = i #sets the POI's community to be highlighted in graphs below\n",
    "        if not found:\n",
    "            print(f'POI not found in this component, highlight defaults to {highlight}')\n",
    "    \n",
    "    def com_plot(self, save=False, plot_st = 0, plot_end=100, fig=(20,15), col='blue'):\n",
    "        a = min(plot_end, len(self.communities))\n",
    "        b = a//5\n",
    "        if b == 0:\n",
    "            b=1\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=fig)\n",
    "        fig.tight_layout()\n",
    "        \n",
    "        \n",
    "        for i in range(plot_st,a): \n",
    "            com = self.proj.subgraph(self.communities[i])\n",
    "            print(i, len(com.nodes), len(com.edges))\n",
    "\n",
    "            plt.subplot(b,5,i+1)\n",
    "            nx.draw_spring(com, node_size=40, node_color=col)\n",
    "        if save:\n",
    "            plt.savefig(f'../Figures/communities.jpg')\n",
    "        \n",
    "    def high_com(self, proj=False, hi_col = 'red', oth_col='blue', crime_col = 'green', save=False):\n",
    "        node_col = []\n",
    "        if proj:\n",
    "            G = self.proj\n",
    "        else:\n",
    "            G = self.G\n",
    "            \n",
    "        for node in G.nodes():\n",
    "            if node in self.c_nodes:\n",
    "                node_col.append(crime_col)\n",
    "            elif node in self.communities[self.highlight]:\n",
    "                node_col.append(hi_col)\n",
    "            else:\n",
    "                node_col.append(oth_col)\n",
    "\n",
    "        \n",
    "        plt.rcParams.update({'figure.figsize': (30, 20)})\n",
    "        pos = nx.spring_layout(G, seed=23)\n",
    "\n",
    "        nx.draw_networkx(G, pos,\n",
    "                         node_size=80,\n",
    "                         node_color=node_col,\n",
    "                         with_labels=False)\n",
    "        \n",
    "        hi_patch = mpatches.Patch(color=hi_col, label=f'Community {self.highlight}')\n",
    "        oth_patch = mpatches.Patch(color=oth_col, label=f'All other nodes')\n",
    "        cri_patch = mpatches.Patch(color=crime_col, label=f'Crime nodes')\n",
    "        if len(c_nodes) > 0 and not proj:\n",
    "            hand = [hi_patch, oth_patch, cri_patch]\n",
    "        else: \n",
    "            hand = [hi_patch, oth_patch]\n",
    "        plt.legend(handles=hand, fontsize='xx-large')\n",
    "         \n",
    "        if save:\n",
    "            plt.savefig(f'../Figures/community_{self.highlight}_highlight.jpg')\n",
    "            \n",
    "    def labled_comm(self, POI_col='aqua', male_col='blue', \n",
    "                    female_col='magenta', crime_col='green',\n",
    "                   vic_col='blue', sus_col='red',\n",
    "                    wit_col='green', other_col='black', save=False):\n",
    "        ed = []\n",
    "        lab = {}\n",
    "        hand1 = []\n",
    "        hand2 = []\n",
    "        \n",
    "        POI_patch = mpatches.Patch(color=POI_col, label=f'POI node')\n",
    "        male_patch = mpatches.Patch(color=male_col, label=f'Male node')\n",
    "        female_patch = mpatches.Patch(color=female_col, label=f'Female node')\n",
    "        crime_patch = mpatches.Patch(color=crime_col, label=f'Crime node')\n",
    "        \n",
    "        vic_line = mlines.Line2D([], [], color=vic_col, label='Victim role')\n",
    "        sus_line = mlines.Line2D([], [], color=sus_col, label='Suspect role')\n",
    "        wit_line = mlines.Line2D([], [], color=wit_col, label='Witness role')\n",
    "        other_line = mlines.Line2D([], [], color=other_col, label='Dual role')\n",
    "        \n",
    "        for per in self.communities[self.highlight]:\n",
    "            if 'p' in per:\n",
    "                lab[per] = (f\"{nx.get_node_attributes(self.G, 'first')[per]} {nx.get_node_attributes(self.G, 'last')[per]} \\n({per})\")\n",
    "                if self.isproj == False:\n",
    "                    for c in self.G.edges([per]):\n",
    "                        ed.append(c[1])\n",
    "                        lab[c[1]] = f'{c[1]}'\n",
    "            else:\n",
    "                lab[per] = f'{per}'\n",
    "\n",
    "        for i in self.communities[self.highlight]:\n",
    "            ed.append(i)\n",
    "  \n",
    "        nc = []\n",
    "        ec = []\n",
    "\n",
    "        hi = self.G.subgraph(ed)\n",
    "\n",
    "        for node in hi:\n",
    "            if node in self.c_nodes:\n",
    "                nc.append(crime_col)\n",
    "                hand1.append(crime_patch)\n",
    "            elif node in self.communities[self.highlight]:\n",
    "                if node == POI:\n",
    "                    nc.append(POI_col)\n",
    "                    hand1.append(POI_patch)\n",
    "                elif nx.get_node_attributes(self.G, 'sex')[node] == '1':\n",
    "                    nc.append(male_col)\n",
    "                    hand1.append(male_patch)\n",
    "                else:\n",
    "                    nc.append(female_col)\n",
    "                    hand1.append(female_patch)\n",
    "            else:\n",
    "                nc.append('yellow') #none should ever show yellow!! something is wrong if they do\n",
    "        \n",
    "        if self.isproj==False:\n",
    "            for edge in hi.edges():\n",
    "                a = nx.get_edge_attributes(hi, 'role')[edge]\n",
    "                if a == 'Victim':\n",
    "                    ec.append(vic_col)\n",
    "                    hand2.append(vic_line)\n",
    "                elif a == 'Suspect':\n",
    "                    ec.append(sus_col)\n",
    "                    hand2.append(sus_line)\n",
    "                elif a == 'Witness':\n",
    "                    ec.append(wit_col)\n",
    "                    hand2.append(wit_line)\n",
    "                else:\n",
    "                    ec.append(other_col)\n",
    "                    hand2.append(other_line)\n",
    "        else:\n",
    "            ec = other_col\n",
    "\n",
    "        hand = list(set(hand1)) + (list(set(hand2)))\n",
    "        plt.rcParams.update({'figure.figsize': (25, 25)})\n",
    "        pos = nx.spring_layout(hi, weight=None, k=0.2, seed=23)\n",
    "\n",
    "        nx.draw_networkx(hi, pos, node_size=200, node_color=nc, edge_color=ec, labels=lab)\n",
    "        plt.legend(handles=hand, fontsize='xx-large')\n",
    "        if save:\n",
    "            plt.savefig(f'../Figures/community_{self.highlight}_focus.jpg')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-resort",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "bet_centrality = nx.betweenness_centrality(P, normalized = True, \n",
    "                                              endpoints = False)\n",
    "bet_cent_list = sorted(bet_centrality.values(), reverse=True)\n",
    "\n",
    "for k,v in bet_centrality.items():\n",
    "    if v == bet_cent_list[0]:\n",
    "        print(f\"POI is set to '{k}'\")\n",
    "        POI = k\n",
    "\n",
    "#POI = 'p56' #set a Person of Interest if not using the highest betweeness centrality person\n",
    "thing = Dunno(G, POI) # will default to using the largest connected component unless you state Gcc=False\n",
    "\n",
    "thing.com_find(proj=True) # set proj=True if you want to find communities for the people projection, else it will use the entire graph you gave it. \n",
    "#print(f'Modularity of network communities: {thing.modularity}')\n",
    "#thing.com_plot(plot_end=10, col='blue') \n",
    "\n",
    "#thing.high_com(hi_col='magenta',oth_col='aqua',) # use proj=True to draw the projected graph, or defaults to drawing the whole graph\n",
    "thing.labled_comm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "swiss-arizona",
   "metadata": {},
   "outputs": [],
   "source": [
    "thing.isproj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "italian-anthropology",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nx.get_node_attributes(G, 'sex')['p583'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greater-commerce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "downtown-cleaners",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check homophily of Criminal_status\n",
    "comm_cs = []\n",
    "for community in thing.communities:\n",
    "    print(len(community))\n",
    "    #print(nx.attribute_assortativity_coefficient(community, \"Criminal_Status\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-keeping",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project onto people and visualise\n",
    "P2 = bipartite.weighted_projected_graph(G0, top_nodes_lcc)\n",
    "\n",
    "print('C',len(P2.nodes()), len(P2.edges()))\n",
    "\n",
    "plt.rcParams.update({'figure.figsize': (30, 20)})\n",
    "pos = nx.spring_layout(P2, seed=23)\n",
    "#nx.draw_networkx(P2, pos, node_size=60, with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "spoken-dutch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find communities in the projected people network\n",
    "communities_p = sorted(comms.greedy_modularity_communities(P2, weight='weight'), key=len, reverse=True) #14 coms modularity 0.813\n",
    "\n",
    "# Count the communities\n",
    "print(f\"There are {len(communities_p)} communities.\")\n",
    "\n",
    "comms.modularity(P2, communities_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abstract-knitting",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(20,10))\n",
    "fig.tight_layout()\n",
    "\n",
    "for i in range(0,10): \n",
    "    com = P2.subgraph(communities_p[i])\n",
    "    node_col = []\n",
    "    for node in com.nodes():\n",
    "        a = nx.get_node_attributes(com, 'Criminal_Status')[node]\n",
    "        if a == 'Criminal':\n",
    "            node_col.append('red')\n",
    "        else:\n",
    "            node_col.append('blue')\n",
    "    print(i, len(com.nodes))\n",
    "    plt.subplot(2,5,i+1)\n",
    "    nx.draw_spring(com, node_size=40, node_color=node_col) #, with_labels=True\n",
    "#plt.savefig(f'../Figures/1-10_C_community.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "victorian-poster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the LCC with Criminals and Victims Color coded\n",
    "pos = nx.spring_layout(G0)\n",
    "plt.figure(3,figsize=(16,16))\n",
    "color_map = []\n",
    "edge_cm = []\n",
    "for node in G0:\n",
    "    if node.startswith('p'):\n",
    "        a = nx.get_node_attributes(G0, 'Criminal_Status')[node]\n",
    "        if a == 'Criminal':\n",
    "            color_map.append('red')\n",
    "        else:\n",
    "            color_map.append('blue')\n",
    "    else:\n",
    "        color_map.append('black')\n",
    "\n",
    "\n",
    "        \n",
    "nx.draw(G0,pos, node_color=color_map, with_labels = False, node_size = 40, width = 0.5)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stuffed-flour",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82a91a22060084f3e8dbe72ba49a002eddd687e7ed0e7d249946fe1b995c1ddc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
