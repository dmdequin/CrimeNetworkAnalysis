{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "sudden-fairy",
   "metadata": {},
   "source": [
    "# Network Analysis Project\n",
    "## Crime Network Analysis\n",
    "Professor: Michele Coscia\n",
    "\n",
    "This notebook contains all of the code developed for the Network Analysis Course at ITU. We will be using...**finish**\n",
    "\n",
    "**Add Description**\n",
    "\n",
    "Group 10:  \n",
    "Carl August Wismer ([cwis@itu.dk](mailto:cwis@itu.dk))  <br>\n",
    "Chrisanna Cornish ([ccor@itu.dk](mailto:ccor@itu.dk))  <br>\n",
    "Danielle Dequin ([ddeq@itu.dk](mailto:ddeq@itu.dk))  <br>\n",
    "Maria Do Carmo Madeira Santos Silva Passos de Sousa ([mdom@itu.dk](mailto:mdom@itu.dk))  <br>\n",
    "Moneeca Abru Iftikhar Latif ([abml@itu.dk](mailto:abml@itu.dk))  <br>\n",
    "Sabrina Fonseca Pereira ([sabf@itu.dk](mailto:sabf@itu.dk))  <br>\n",
    "\n",
    "Created: 27-09-2021  \n",
    "Last Modified: 29-11-2021"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tracked-butler",
   "metadata": {},
   "source": [
    "# Data Source\n",
    "\n",
    "The data are a network of associations among suspects, victims, and/or witnesses involved in crimes in St. Louis in the 1990s. Data are derived from police records, via snowball sampling from five initial homicides. Left nodes are people, right nodes are crime events, and edges connect people to particular crimes events they were associated with. Metadata includes names, genders, and roles (suspects, victims, and/or witnesses).\n",
    "\n",
    "This is an undirected, unweighted, bipartite network with 1380 nodes and 1476 edges.\n",
    "\n",
    "Data can be downloaded [here](http://konect.cc/networks/moreno_crime/) or [here](https://networks.skewed.de/net/crime)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-estate",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "voluntary-martial",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from networkx.algorithms import bipartite\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats import linregress, pearsonr\n",
    "from scipy.optimize import curve_fit\n",
    "import powerlaw as pl\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immediate-boxing",
   "metadata": {},
   "source": [
    "# Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "million-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"Function to normalize the y count of the degrees, so that the y-values on the plots\n",
    "    will be a probability of that degree, instead of the count of the degree.\"\"\"\n",
    "    total = sum(x.values())\n",
    "    x = {\n",
    "           k: v / total\n",
    "           for k, v in x.items()\n",
    "        }\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-spectacular",
   "metadata": {},
   "source": [
    "# PATHS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-thermal",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA = '../Data/out.moreno_crime_crime'\n",
    "ROLE = '../Data/rel.moreno_crime_crime.person.role'\n",
    "NAME = '../Data/ent.moreno_crime_crime.person.name'\n",
    "SEX =  '../Data/ent.moreno_crime_crime.person.sex'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "voluntary-jefferson",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adult-hampton",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(DATA) as f:\n",
    "    data = f.read().splitlines()\n",
    "    \n",
    "with open(ROLE) as f:\n",
    "    role = f.read().splitlines()\n",
    "    \n",
    "with open(NAME) as f:\n",
    "    name = f.read().splitlines()\n",
    "    \n",
    "with open(SEX) as f:\n",
    "    sex = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-girlfriend",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seperates first and last names so we can make it look pretty later.\n",
    "lasts = []\n",
    "firsts = []\n",
    "for n in name:\n",
    "    lasts.append((re.findall('[A-Z][^A-Z]*', n))[0]) \n",
    "    firsts.append((re.findall('[A-Z][^A-Z]*', n))[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-remark",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = np.loadtxt(DATA, dtype=str) \n",
    "\n",
    "for i in graph:\n",
    "    if len(i[0]) == 1:\n",
    "        i[0] = '00' + i[0]\n",
    "    elif len(i[0]) == 2:\n",
    "        i[0] = '0' + i[0]\n",
    "    #else:\n",
    "        #i[0] = '' + i[0]\n",
    "for i in graph:        \n",
    "    if len(i[1]) == 1:\n",
    "        i[1] = '00' + i[1]\n",
    "    elif len(i[1]) == 2:\n",
    "        i[1] = '0' + i[1]\n",
    "    \n",
    "df = pd.DataFrame(graph)\n",
    "df.columns = [\"person\", \"crime\"]\n",
    "df[\"person\"] = 'p' + df['person'] \n",
    "\n",
    "edges = [e for e in zip(df['person'], df['crime'])]\n",
    "\n",
    "a = np.unique(graph[:,0]) # people nodes\n",
    "b = np.unique(graph[:,1]) # crime nodes\n",
    "a = ['p' + a for a in a] # add string to differentiate\n",
    "print(f\"There are {len(a)} unique people and {len(b)} unique crime events.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-twelve",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sanity check to make sure the data is being loaded correctly.\n",
    "first_10_roles = ['Suspect', 'Victim', 'Victim', 'Suspect', 'Victim', 'Suspect', 'Victim', 'Suspect', 'Suspect', 'Suspect']\n",
    "\n",
    "for i in range(10):\n",
    "    print(edges[i], data[i], role[i], first_10_roles[i]) # to verify it hasn't changed the order of the edge list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "opening-morning",
   "metadata": {},
   "source": [
    "## Create Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-combine",
   "metadata": {},
   "outputs": [],
   "source": [
    "G = nx.Graph()\n",
    "\n",
    "# add nodes\n",
    "for j in range(len(a)):\n",
    "    G.add_node(a[j], bipartite=0, data=True, first=firsts[j], last=lasts[j], sex=sex[j]) # Add attributes name and sex\n",
    "\n",
    "G.add_nodes_from(b, bipartite=1, data=True)\n",
    "\n",
    "# add edges\n",
    "i = 0\n",
    "for edge in edges:\n",
    "    G.add_edge(edge[0], edge[1], role=role[i])\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "royal-geneva",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "print(\n",
    "    'Num. of nodes: {} \\nNum. of edges: {} \\nIs bipartite? {} \\nIs connected? {}'.format(\n",
    "        G.number_of_nodes(), \n",
    "        G.number_of_edges(), \n",
    "        nx.is_bipartite(G),\n",
    "        nx.is_connected(G)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "satisfied-hours",
   "metadata": {},
   "source": [
    "From the [official Networkx docs](https://networkx.org/documentation/stable/reference/algorithms/bipartite.html):\n",
    "\n",
    "Many algorithms of the bipartite module of NetworkX require, as an argument, a container with all the nodes that belong to one set, in addition to the bipartite graph B. The functions in the bipartite package do not check that the node set is actually correct nor that the input graph is actually bipartite. If B is connected, you can find the two node sets using a two-coloring algorithm:\n",
    "\n",
    "```py\n",
    ">>> nx.is_connected(B)\n",
    "True\n",
    ">>> bottom_nodes, top_nodes = bipartite.sets(B)\n",
    "```\n",
    "However, if the input graph is not connected, there are more than one possible colorations. This is the reason why we require the user to pass a container with all nodes of one bipartite node set as an argument to most bipartite functions. In the face of ambiguity, we refuse the temptation to guess and raise an AmbiguousSolution Exception if the input graph for bipartite.sets is disconnected.\n",
    "\n",
    "Using the bipartite node attribute, you can easily get the two node sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "equivalent-surname",
   "metadata": {},
   "outputs": [],
   "source": [
    "top_nodes = {n for n, d in G.nodes(data=True) if d[\"bipartite\"] == 0} # people\n",
    "bottom_nodes = set(G) - top_nodes # crimes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comfortable-premises",
   "metadata": {},
   "source": [
    "## Adding metadata to nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-carpet",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{nx.get_node_attributes(G, 'first')['p001']} {nx.get_node_attributes(G, 'last')['p001']}\") # check name of person 'p001' = 'Dennis Abel'\n",
    "print(nx.get_node_attributes(G, 'sex')['p001']) # check sex of person 'p001' = '1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "included-aspect",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.get_edge_attributes(G, 'role')['p001', '001']\n",
    "# check role of edge between person p1 and crime 1 = 'Suspect'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consolidated-capacity",
   "metadata": {},
   "source": [
    "### Add \"Criminal Status\" node attribute to each people node\n",
    "\n",
    "**Here, we are making the assumption that, if a person has been involved in a crime as a \"suspect\" or \"victim suspect\" then they are a criminal. Meanwhile, if someone has only been a \"witness\" or a \"victim\" then they are \"innocent\".**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-consequence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a dictionary based on people nodes to keep track of all roles per node\n",
    "p_nodes = {el:[] for el in top_nodes}\n",
    "# Add all the edge attributes to a dictionary of people nodes\n",
    "for key,value in nx.get_edge_attributes(G, 'role').items():\n",
    "    for part in key:\n",
    "        if part in top_nodes:\n",
    "            p_nodes[part].append(value)\n",
    "print(p_nodes['p001']) # List of all roles from p001\n",
    "\n",
    "# Initialize a dictionary to keep track of who is a \"criminal\"\n",
    "criminals = {el:[] for el in top_nodes}\n",
    "# Loop through all roles per node, and deem them criminals if ever they have been a suspect\n",
    "for key in p_nodes:\n",
    "    for value in p_nodes[key]:\n",
    "        if value == \"Suspect\":\n",
    "            criminals[key] = \"Criminal\"\n",
    "            break\n",
    "        elif value == \"Victim Suspect\":\n",
    "            criminals[key] = \"Criminal\"\n",
    "            break\n",
    "        else:\n",
    "            criminals[key] = \"Innocent\"\n",
    "            \n",
    "print(criminals['p001']) # Verify that p1 is deemed \"criminal\"\n",
    "\n",
    "# Convert to pandas df\n",
    "criminals_df = pd.DataFrame(criminals.items(), columns=['node', 'Criminal_Status'])\n",
    "\n",
    "# loop through rows in the data frame and add the attribute of Criminal Status\n",
    "for index, row in criminals_df.iterrows():\n",
    "    G.nodes[row['node']]['Criminal_Status'] = row['Criminal_Status']\n",
    "\n",
    "print(nx.get_node_attributes(G, 'Criminal_Status')['p001']) # check name of person 'p001' = 'Criminal'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "italian-greece",
   "metadata": {},
   "source": [
    "# Finding Largest Connected Component"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "solar-adelaide",
   "metadata": {},
   "source": [
    "### *LCC:* Here we find that the largest connected component contains 1263 nodes (out of 1380 total nodes). This has a large majority of the nodes in the connected component. We do futher analysis to see if we should focus on this component alone as we continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-shore",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components in the graph sorted in descendent order\n",
    "Gcc = sorted(nx.connected_components(G), key=len, reverse=True)\n",
    "\n",
    "# Selecting the biggest component\n",
    "G0 = G.subgraph(Gcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lists of nodes for the LCC\n",
    "top_nodes_lcc = {n for n, d in G0.nodes(data=True) if d[\"bipartite\"] == 0} # people\n",
    "bottom_nodes_lcc = set(G0) - top_nodes_lcc      # crimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "scientific-colony",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes for each component and number of components\n",
    "comp_list = []\n",
    "for comp in Gcc:\n",
    "    comp_len = len(comp)\n",
    "    comp_list.append(comp_len)\n",
    "\n",
    "BC_perc = round(comp_list[0]/sum(comp_list) * 100,3)\n",
    "\n",
    "print(f'Number of nodes for each components: {comp_list} and number of components: {len(comp_list)}.')\n",
    "print(f'The number of number of nodes in the biggest component represents {BC_perc} % of all nodes.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "precious-volunteer",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "intense-mailing",
   "metadata": {},
   "source": [
    "## Average Clustering\n",
    "### *Clustering coefficient:* number of common neighbors around a node connected to each other. How often does a triad (set of three connected nodes) closes down into a triangle (three nodes that are all connected to each other)? \n",
    "#### Note_1: in the formula, we multiply by 3 because a single triangle closes three triads.\n",
    "#### Note_2: local, global and average clustering coefficients are different.\n",
    "\n",
    "In our case, almost half of the triads (0.43) in the network close to form a triangle. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "north-alliance",
   "metadata": {},
   "source": [
    "### *Density:* probability that a random node pair is connected or, the number of edges in a network over the total possible number of edges that can exist given the number of nodes. \n",
    "\n",
    "In our case, we have a probablity of 0.003 which means that our network is quite sparse. Additionally, we see that the probability is very similar for the LCC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "moderate-concert",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Density: {} \\nAvarage clustering: {}'.format(\n",
    "        round(bipartite.density(G, bottom_nodes),3),\n",
    "        round(bipartite.average_clustering(G),3)\n",
    "        )\n",
    "    )\n",
    "print('\\n')\n",
    "print('Density LCC: {} \\nAverage Clustering LCC: {}'.format(\n",
    "        round(bipartite.density(G0, bottom_nodes_lcc),3), \n",
    "        round(bipartite.average_clustering(G0),3)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "underlying-microwave",
   "metadata": {},
   "source": [
    "### **Looking for both density and clustering coefficient values, our network follows the real world network characterists having few connections per node, but these connections tend to be clustered in the same neighborhood. Nodes tend to close triangles.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "competitive-shower",
   "metadata": {},
   "source": [
    "## Global Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-sector",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the entire graph\n",
    "global_clustering = bipartite.robins_alexander_clustering(G)\n",
    "print(f'Global Clustering: {round(global_clustering,4)}')\n",
    "\n",
    "# This is for only the LCC\n",
    "global_clustering_lcc = bipartite.robins_alexander_clustering(G0)\n",
    "print(f'Global Clustering LCC: {round(global_clustering_lcc, 4)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "offensive-judges",
   "metadata": {},
   "source": [
    "# Nodes stats"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "piano-davis",
   "metadata": {},
   "source": [
    "### *Betweenness centrality:* total number of paths that can pass through a node – excluding the ones for which it is the origin or the destination; how many paths would become longer if node v would disappear from the network?\n",
    "\n",
    "The pattern of betweenness centrality between the LCC and the entire network is very similar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-signal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Whole network\n",
    "betweenness_centrality = bipartite.betweenness_centrality(G, top_nodes)\n",
    "\n",
    "# For the LCC\n",
    "betweenness_centrality_lcc = bipartite.betweenness_centrality(G0, top_nodes_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "frank-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person with the highest betweenness centrality in entire network AND LCC\n",
    "betweenness_centrality['p815'], betweenness_centrality_lcc['p815']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "broken-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Person with highest betweenness centrality in the largest community\n",
    "betweenness_centrality['p056'], betweenness_centrality_lcc['p056']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "collective-cross",
   "metadata": {},
   "outputs": [],
   "source": [
    "max(betweenness_centrality.values()), max(betweenness_centrality_lcc.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informed-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1.set_title('Betweenness Centrality of Whole Network')\n",
    "ax2.set_title('Betweenness Centrality of LCC')\n",
    "ax1.hist(betweenness_centrality.values(), bins=10, log=True)\n",
    "ax2.hist(betweenness_centrality_lcc.values(), bins=10, log=True)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annoying-wealth",
   "metadata": {},
   "source": [
    "### *Closeness centrality:* inverse average path length. The closer a node are on average to every other nodes, the more central it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "preceding-judges",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the Whole Network\n",
    "closeness_centrality = bipartite.closeness_centrality(G, top_nodes)\n",
    "\n",
    "# For the LCC\n",
    "closeness_centrality_lcc = bipartite.closeness_centrality(G0, top_nodes_lcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "maritime-minnesota",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1,2, figsize=(10,4))\n",
    "ax1.set_title('Closeness Centrality of Whole Network')\n",
    "ax2.set_title('Closeness Centrality of LCC')\n",
    "ax1.hist(closeness_centrality.values(), bins=900)\n",
    "ax1.set_xlim(.06,.19)\n",
    "ax2.hist(closeness_centrality_lcc.values(), bins=75)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floppy-substitute",
   "metadata": {},
   "source": [
    "### *Avg degree:* average number of edges each node uses to connect to its neighbors. Depending on the number of nodes and the avg degree, we can have an idea of how connected or sparse our network is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "loose-magazine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Degree\n",
    "node_degree = bipartite.degrees(G, top_nodes)\n",
    "#print(node_degree[1]) \n",
    "# node_degree[1] is showing the people nodes, and their corresponding degree\n",
    "\n",
    "node_deg_dict = {}\n",
    "\n",
    "for i in node_degree[1]:\n",
    "    node_deg_dict[i[0]] = i[1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assumed-choice",
   "metadata": {},
   "outputs": [],
   "source": [
    "# People node with the most neighbors (Max left degree)\n",
    "node_neighbors = {n: len(set(G.neighbors(n))) for n in top_nodes}\n",
    "maxnode = max(node_neighbors, key = node_neighbors.get)\n",
    "print(maxnode, node_neighbors[maxnode])\n",
    "\n",
    "# People node in LCC with most neighbors\n",
    "node_neighbors_lcc = {n: len(set(G0.neighbors(n))) for n in top_nodes_lcc}\n",
    "maxnode_lcc = max(node_neighbors_lcc, key = node_neighbors_lcc.get)\n",
    "print(maxnode_lcc, node_neighbors_lcc[maxnode_lcc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "selective-kentucky",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime node with the most neighbors (Max right degree)\n",
    "node_neighbors = {n: len(set(G.neighbors(n))) for n in bottom_nodes}\n",
    "maxnode = max(node_neighbors, key = node_neighbors.get)\n",
    "print(maxnode, node_neighbors[maxnode])\n",
    "\n",
    "node_neighbors_lcc = {n: len(set(G0.neighbors(n))) for n in bottom_nodes_lcc}\n",
    "maxnode_lcc = max(node_neighbors_lcc, key = node_neighbors_lcc.get)\n",
    "print(maxnode_lcc, node_neighbors_lcc[maxnode_lcc])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "referenced-connectivity",
   "metadata": {},
   "source": [
    "# Degree Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "binary-bunny",
   "metadata": {},
   "source": [
    "### *This is looking into the degree distribution of all nodes in the entire network and all nodes of the largest connected component to compare the entire network with the LCC. A loglog is applied to both to see if they follow a power law. The y axis shows the probability of a node having a degree equal to k.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "czech-stamp",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree Distribution of the Entire Network, Combining People and Crime Nodes\n",
    "dd = Counter(dict(G.degree).values())\n",
    "dd = pd.DataFrame(list(dd.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "dd[\"p(k)\"] = dd[\"p(k)\"]/(dd[\"p(k)\"].sum())\n",
    "\n",
    "# Degree Distribution of the LCC, Combining People and Crime Nodes\n",
    "dd_lcc = Counter(dict(G0.degree).values())\n",
    "dd_lcc = pd.DataFrame(list(dd_lcc.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "dd_lcc[\"p(k)\"] = dd_lcc[\"p(k)\"]/(dd_lcc[\"p(k)\"].sum())\n",
    "\n",
    "fig, axs = plt.subplots(2,2, figsize=(12,10)) \n",
    "dd.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", ax=axs[0,0], color = \"#e41a1c\", title=\"Degree Distribution of All Nodes\")\n",
    "dd.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", ax=axs[0,1], color = \"#e41a1c\", loglog = True, title=\"Loglog Degree Distribution of All Nodes\")\n",
    "\n",
    "dd_lcc.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", ax=axs[1,0], color = \"#e41a1c\", title=\"Degree Distribution of LCC\")\n",
    "dd_lcc.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", ax=axs[1,1], color = \"#e41a1c\", loglog = True, title=\"Loglog Degree Distribution of LCC\")\n",
    "#plt.savefig(\"../Report/degree_distributions.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attended-street",
   "metadata": {},
   "source": [
    "## People Vs Crime Node Degree Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "revised-binding",
   "metadata": {},
   "source": [
    "### *This is looking into the degree distribution of the crime nodes and the people nodes separtely, to see if one of them stands out as having a very different distribution. This is broken down into the Entire Network, as well as the Largest-connected-component of the entire network. In all cases, it seems like the degree distribution is fat-tailed, with very few nodes having a high degree, and most nodes having a very low degree. Meaning most nodes are connected to very few other nodes. In other words, most people are involved in very few crimes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "becoming-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes the entire graph\n",
    "e_p = []\n",
    "e_c = []\n",
    "for k in G.nodes(): # Loop through all nodes\n",
    "    if G.nodes[k][\"bipartite\"] == 0: # If People Nodes\n",
    "        e_p.append(len(G.edges(k)))  # Append number of edges to each node\n",
    "    else:                            # If Crime Nodes\n",
    "        e_c.append(len(G.edges(k)))  # Append number of edges to each crime\n",
    "\n",
    "e_cc = Counter(e_c)\n",
    "e_cc= normalize(e_cc)\n",
    "\n",
    "e_pp = Counter(e_p)\n",
    "e_pp = normalize(e_pp)\n",
    "\n",
    "fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2,2, figsize=(12,10))\n",
    "fig.suptitle('Degree Distributions of the Network')\n",
    "\n",
    "ax1.scatter(e_cc.keys(), e_cc.values())\n",
    "ax1.set_title('Crime Nodes')\n",
    "ax1.set_xlabel('Degree (k)')\n",
    "ax1.set_ylabel('p(k)')\n",
    "\n",
    "ax2.scatter(e_pp.keys(), e_pp.values())\n",
    "ax2.set_title('People Nodes')\n",
    "ax2.set_xlabel('Degree (k)')\n",
    "ax2.set_ylabel('p(k)');\n",
    "\n",
    "# This only deals with the LCC\n",
    "# Lists of number of edges per node\n",
    "e_p1 = [] # Number of edges connected to each People node\n",
    "e_c1 = [] # Number of edges connected to each Crime node\n",
    "for k in G0.nodes(): # loop through all nodes\n",
    "    if G0.nodes[k][\"bipartite\"] == 0:  # If People Node\n",
    "        e_p1.append(len(G0.edges(k)))  # Append number of edges to each node\n",
    "    else:                              # If Crime Node\n",
    "        e_c1.append(len(G0.edges(k)))  # Append number of edges to each node\n",
    "\n",
    "e_cc1 = Counter(e_c1)\n",
    "e_cc1 = normalize(e_cc1)\n",
    "\n",
    "e_pp1 = Counter(e_p1)\n",
    "e_pp1 = normalize(e_pp1)\n",
    "\n",
    "ax3.scatter(e_cc1.keys(), e_cc1.values())\n",
    "ax3.set_title('Crime Nodes of LCC')\n",
    "ax3.set_xlabel('Degree (k)')\n",
    "ax3.set_ylabel('p(k)')\n",
    "\n",
    "ax4.scatter(e_pp1.keys(), e_pp1.values())\n",
    "ax4.set_title('People Nodes of LCC')\n",
    "ax4.set_xlabel('Degree (k)')\n",
    "ax4.set_ylabel('p(k)');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "shared-sunrise",
   "metadata": {},
   "source": [
    "# CCDF\n",
    "### *The Complement of a cumulative distribution function (CCDF) is the probability of finding a node of degree k or higher. It appears that, in log-log space, the relationship between degree and the number of nodes with a given degree is fixed.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exterior-zimbabwe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cumulative Degree distribution\n",
    "ccdf = dd.sort_values(by = \"Degree (k)\", ascending = False)\n",
    "ccdf[\"cumsum\"] = ccdf[\"p(k)\"].cumsum()\n",
    "ccdf[\"ccdf: P(X>=k)\"] = ccdf[\"cumsum\"] / ccdf[\"p(k)\"].sum()\n",
    "ccdf = ccdf[[\"Degree (k)\", \"ccdf: P(X>=k)\"]].sort_values(by = \"Degree (k)\")\n",
    "\n",
    "ccdf.plot(kind = \"line\", x = \"Degree (k)\", y = \"ccdf: P(X>=k)\", color = \"#e41a1c\", loglog = True)\n",
    "#plt.savefig(\"../Report/degree_distribution_ccdf.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-proposition",
   "metadata": {},
   "source": [
    "## Fit CCDF to PowerLaw\n",
    "### *A power law is a functional relationship between two quantities, where a relative change in one quantity results in a proportional relative change in the other quantity.*\n",
    "\n",
    "https://pythonhosted.org/powerlaw/\n",
    "\n",
    "$p(k) \\approx k^{-\\alpha}$\n",
    "\n",
    "$y = kx^{\\alpha}$\n",
    "\n",
    "Where: \n",
    "X and Y are variables of interest,\n",
    "$\\alpha$ is the law’s exponent,\n",
    "$k$ is a constant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-paradise",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the log of the ccdf\n",
    "logcdf = np.log10(ccdf[[\"Degree (k)\", \"ccdf: P(X>=k)\"]])\n",
    "# find the slope, intercept, r-value, and p-value using linear regression\n",
    "slope, log10intercept, r_value, p_value, std_err = linregress(logcdf[\"Degree (k)\"], logcdf[\"ccdf: P(X>=k)\"])\n",
    "\n",
    "print(\"CCDF Fit: %1.4f x ^ %1.4f (R2 = %1.4f, p = %1.4f)\" % (10 ** log10intercept, slope, r_value ** 2, p_value))\n",
    "print(\"\\n\")\n",
    "\n",
    "# fit the data to the power law\n",
    "results = pl.Fit(ccdf[\"ccdf: P(X>=k)\"])\n",
    "# Select data above a specific degree\n",
    "k_min = ccdf[ccdf[\"ccdf: P(X>=k)\"] == results.power_law.xmin][\"Degree (k)\"]\n",
    "\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polish-hospital",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best fit.\n",
    "ccdf[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf[\"Degree (k)\"] ** -results.power_law.alpha)\n",
    "ax = plt.gca()\n",
    "ccdf.plot(kind = \"line\", x = \"Degree (k)\", y = \"ccdf: P(X>=k)\", color = \"#e41a1c\", loglog = True, ax = ax)\n",
    "ccdf.plot(kind = \"line\", x = \"Degree (k)\", y = \"fit\", color = \"#377eb8\", loglog = True, ax = ax)\n",
    "#plt.savefig(\"../Report/ccdf_fit.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "entire-finding",
   "metadata": {},
   "source": [
    "# Degree Distributions Per Node for Entire Network\n",
    "### *Again, comparing degree distributions of the entire network, with people and crime nodes separated, but on the same axis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formed-assembly",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes the entire network\n",
    "\n",
    "# Dictionary of Crime Nodes and Their Degree\n",
    "crimes_degree = list(G.degree(b))\n",
    "crimes_degree = pd.DataFrame(crimes_degree, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Dictionary of Crime Node Degrees and Their Frequencies\n",
    "crimes_degree_count = Counter(dict(G.degree(b)).values())\n",
    "crimes_degree_count = pd.DataFrame(list(crimes_degree_count.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "crimes_degree_count[\"p(k)\"] = crimes_degree_count[\"p(k)\"]/(crimes_degree_count[\"p(k)\"].sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unnecessary-batman",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes the entire network\n",
    "\n",
    "# Dictionary of People Nodes and Their Degree\n",
    "ppl_degree = list(G.degree(a))\n",
    "ppl_degree = pd.DataFrame(ppl_degree, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Dictionary of People Node Degrees and Their Frequencies\n",
    "ppl_degree_count = Counter(dict(G.degree(a)).values())\n",
    "ppl_degree_count = pd.DataFrame(list(ppl_degree_count.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "ppl_degree_count[\"p(k)\"] = ppl_degree_count[\"p(k)\"]/(ppl_degree_count[\"p(k)\"].sum())\n",
    "\n",
    "#ppl_degree_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nearby-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This includes the entire network\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "crimes_degree_count.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", color = \"#fb00ff\", ax = ax, alpha=0.9, s=200)\n",
    "ppl_degree_count.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", color = \"#ff9d00\", ax = ax, alpha=0.7, s=200)\n",
    "ax.legend([\"Crimes\",\"People\"])\n",
    "plt.xticks(np.arange(0, 26, step=1))\n",
    "plt.title(\"Degree Distributions per Node Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aggressive-capability",
   "metadata": {},
   "source": [
    "# Degree Distribution Using only the LCC\n",
    "### *Now comparing the degree distributions of the people and crime nodes in the largest connected component on one axis. Here it follows the same pattern as that of the entire network, indicating that focusing on the largest connected component alone will maintain the integrity of the analysis on the entire network. In other words, the results on the LCC can be generalized towards the entire network, as it represents the entire network well.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specified-contract",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of People Nodes and their Degree\n",
    "p_deg_lcc = list(G0.degree(a))\n",
    "p_deg_lcc = pd.DataFrame(p_deg_lcc, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# DataFrame of People Node Degrees and their Frequency\n",
    "p_deg_lcc_count = Counter(dict(G0.degree(a)).values())\n",
    "p_deg_lcc_count = pd.DataFrame(list(p_deg_lcc_count.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "p_deg_lcc_count[\"p(k)\"] = p_deg_lcc_count[\"p(k)\"]/(p_deg_lcc_count[\"p(k)\"].sum())\n",
    "\n",
    "#p_deg_lcc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-protein",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame of Crime Nodes and their Degree\n",
    "c_deg_lcc = list(G0.degree(b))\n",
    "c_deg_lcc = pd.DataFrame(c_deg_lcc, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Create DataFrame of Crime Node Degrees and their Frequency\n",
    "c_deg_lcc_count = Counter(dict(G0.degree(b)).values())\n",
    "c_deg_lcc_count = pd.DataFrame(list(c_deg_lcc_count.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "c_deg_lcc_count[\"p(k)\"] = c_deg_lcc_count[\"p(k)\"]/(c_deg_lcc_count[\"p(k)\"].sum())\n",
    "\n",
    "#crimes_degree_lcc_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Uses Only the LCC\n",
    "fig, ax = plt.subplots(figsize=(8,5))\n",
    "ax = plt.gca()\n",
    "c_deg_lcc_count.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", color = \"#fb00ff\", ax = ax, alpha=0.9, s=150)\n",
    "p_deg_lcc_count.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", color = \"#ff9d00\", ax = ax, alpha=0.7, s=150)\n",
    "\n",
    "ax.legend([\"Crimes\",\"People\"])\n",
    "plt.xticks(np.arange(0, 26, step=1))\n",
    "plt.title(\"Degree Distributions per Node Category Using Only LCC\")\n",
    "#plt.savefig(\"../Report/degree_distribution_lcc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-brass",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Uses Only the LCC\n",
    "fig, ax = plt.subplots(figsize=(8,5)) \n",
    "ax = plt.gca()\n",
    "c_deg_lcc_count.plot(kind = \"scatter\", loglog=True, x = \"Degree (k)\", y = \"p(k)\", color = \"#fb00ff\", ax = ax, alpha=0.9, s=150)\n",
    "p_deg_lcc_count.plot(kind = \"scatter\", loglog=True, x = \"Degree (k)\", y = \"p(k)\", color = \"#ff9d00\", ax = ax, alpha=0.7, s=150)\n",
    "\n",
    "ax.legend([\"Crimes\",\"People\"])\n",
    "plt.title(\"Degree Distributions per Node Category Using Only LCC\")\n",
    "#plt.savefig(\"../Report/degree_distribution_loglog_lcc.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lightweight-mexico",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Degree distribution of Both Nodes in LCC\n",
    "ccdf_lcc = dd_lcc.sort_values(by = \"Degree (k)\", ascending = False)\n",
    "ccdf_lcc[\"cumsum\"] = ccdf_lcc[\"p(k)\"].cumsum()\n",
    "ccdf_lcc[\"ccdf: P(X>=k)\"] = ccdf_lcc[\"cumsum\"] / ccdf_lcc[\"p(k)\"].sum()\n",
    "ccdf_lcc = ccdf_lcc[[\"Degree (k)\", \"ccdf: P(X>=k)\"]].sort_values(by = \"Degree (k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mighty-eagle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crime Degree Distribution Using Only LCC\n",
    "c_ccdf = c_deg_lcc_count.sort_values(by = \"Degree (k)\", ascending = False)\n",
    "c_ccdf[\"cumsum\"] = c_ccdf[\"p(k)\"].cumsum()\n",
    "c_ccdf[\"ccdf: P(X>=k)\"] = c_ccdf[\"cumsum\"] / c_ccdf[\"p(k)\"].sum()\n",
    "c_ccdf = c_ccdf[[\"Degree (k)\", \"ccdf: P(X>=k)\"]].sort_values(by = \"Degree (k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "instant-bullet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# People Degree Distribution Using Only LCC\n",
    "p_ccdf = p_deg_lcc_count.sort_values(by = \"Degree (k)\", ascending = False)\n",
    "p_ccdf[\"cumsum\"] = p_ccdf[\"p(k)\"].cumsum()\n",
    "p_ccdf[\"ccdf: P(X>=k)\"] = p_ccdf[\"cumsum\"] / p_ccdf[\"p(k)\"].sum()\n",
    "p_ccdf = p_ccdf[[\"Degree (k)\", \"ccdf: P(X>=k)\"]].sort_values(by = \"Degree (k)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-picking",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1,2, figsize=(12,4)) \n",
    "ax = plt.gca()\n",
    "\n",
    "c_ccdf.plot(kind = \"line\", x = \"Degree (k)\", y = \"ccdf: P(X>=k)\", ax=axs[0], color = \"#fb00ff\", loglog = True)\n",
    "p_ccdf.plot(kind = \"line\", x = \"Degree (k)\", y = \"ccdf: P(X>=k)\", ax=axs[0], color = \"#ff9d00\", loglog = True)\n",
    "axs[0].legend([\"Crimes\",\"People\"])\n",
    "axs[0].set_title(\"CCDF per Node Category Using Only LCC\")\n",
    "\n",
    "ccdf_lcc.plot(kind = \"line\", x = \"Degree (k)\", y = \"ccdf: P(X>=k)\", ax=axs[1], color = \"r\", loglog = True)\n",
    "axs[1].legend([\"Both Nodes Combined\"])\n",
    "axs[1].set_title(\"CCDF For All Nodes in LCC\")\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "charitable-advisory",
   "metadata": {},
   "source": [
    "# Fit CCDF for LCC to PowerLaw\n",
    "## *Explanation*..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "related-california",
   "metadata": {},
   "outputs": [],
   "source": [
    "logcdf_lcc = np.log10(ccdf_lcc[[\"Degree (k)\", \"ccdf: P(X>=k)\"]])\n",
    "slope, log10intercept, r_value, p_value, std_err = linregress(logcdf_lcc[\"Degree (k)\"], logcdf_lcc[\"ccdf: P(X>=k)\"])\n",
    "print(\"CCDF Fit: %1.4f x ^ %1.4f (R2 = %1.4f, p = %1.4f)\" % (10 ** log10intercept, slope, r_value ** 2, p_value))\n",
    "print(\"\\n\")\n",
    "\n",
    "results = pl.Fit(ccdf_lcc[\"ccdf: P(X>=k)\"])\n",
    "k_min = ccdf_lcc[ccdf_lcc[\"ccdf: P(X>=k)\"] == results.power_law.xmin][\"Degree (k)\"]\n",
    "print(\"Powerlaw CCDF Fit: %1.4f x ^ -%1.4f (k_min = %d)\" % (10 ** results.power_law.Kappa, results.power_law.alpha, k_min))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-torture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the best fit.\n",
    "ccdf_lcc[\"fit\"] = (10 ** results.power_law.Kappa) * (ccdf_lcc[\"Degree (k)\"] ** -results.power_law.alpha)\n",
    "ax = plt.gca()\n",
    "ccdf_lcc.plot(kind = \"line\", x = \"Degree (k)\", y = \"ccdf: P(X>=k)\", color = \"#e41a1c\", loglog = True, ax = ax)\n",
    "ccdf_lcc.plot(kind = \"line\", x = \"Degree (k)\", y = \"fit\", color = \"#377eb8\", loglog = True, ax = ax)\n",
    "#plt.savefig(\"../Report/ccdf_lcc_fit.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minor-essay",
   "metadata": {},
   "source": [
    "# Projected Plots of the Entire Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "built-wages",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Entire Network Projected onto People\n",
    "P = bipartite.weighted_projected_graph(G, top_nodes)\n",
    "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
    "pos = nx.spring_layout(P, seed=42)\n",
    "nx.draw_networkx(P, pos, node_size=20, with_labels=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifteen-matrix",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Entire Network projected onto Crimes\n",
    "C = bipartite.weighted_projected_graph(G, bottom_nodes)\n",
    "plt.rcParams.update({'figure.figsize': (20, 10)})\n",
    "pos = nx.spring_layout(C, seed=42)\n",
    "nx.draw_networkx(C, pos, node_size=20, with_labels=False)\n",
    "#nx.draw(C, node_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ranging-trustee",
   "metadata": {},
   "source": [
    "# Plots of Largest Connected Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informational-popularity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot only biggest component\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "pos = nx.spring_layout(G0)\n",
    "posB = nx.bipartite_layout(G0, top_nodes)\n",
    "biggest_comp_graph = nx.draw(G0, pos, node_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "identical-gregory",
   "metadata": {},
   "source": [
    "## Projected Graphs From Only the LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-ratio",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Just the People from the LCC\n",
    "P = bipartite.weighted_projected_graph(G0, top_nodes_lcc)\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "pos = nx.spring_layout(P, seed=42)\n",
    "nx.draw_networkx(P, pos, node_size=20, with_labels=False)\n",
    "\n",
    "#nx.draw(P, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hollow-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT IS THIS?????\n",
    "# node = G0.nodes['p271']\n",
    "x = list(G0.neighbors('p715'))\n",
    "#for i in x:\n",
    "#    print(i, list(G0.neighbors(i)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT IS THIS????\n",
    "from collections import defaultdict\n",
    "con = defaultdict(int)\n",
    "for a,b in P.edges:\n",
    "    con[a] += 1\n",
    "    con[b] += 1\n",
    "#print(sorted(con.values(), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-monster",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only Crimes from the LCC\n",
    "C = bipartite.weighted_projected_graph(G0, bottom_nodes_lcc)\n",
    "plt.rcParams.update({'figure.figsize': (15, 10)})\n",
    "pos = nx.spring_layout(C, seed=42)\n",
    "nx.draw_networkx(C, pos, node_size=20, with_labels=False)\n",
    "\n",
    "#nx.draw(C, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "higher-shift",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Biggest component bipartite plot\n",
    "nx.draw_networkx(G0, pos = nx.drawing.layout.bipartite_layout(G0, top_nodes), \n",
    "                 width = 0.1, linewidths = 0.5, node_size = 0.5, with_labels = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "constant-habitat",
   "metadata": {},
   "source": [
    "## Non-square adjacency matrix, and stocastic matrix\n",
    "\n",
    "## Add explanation..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perceived-principle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non-square adjacency matrix\n",
    "adjmat = nx.algorithms.bipartite.matrix.biadjacency_matrix(G, top_nodes)\n",
    "print(adjmat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "available-blond",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Project along smaller axis\n",
    "if adjmat.shape[0] == 551:\n",
    "    adjmat_proj = adjmat.dot(adjmat.T)\n",
    "else:\n",
    "    adjmat_proj = adjmat.T.dot(adjmat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "judicial-playback",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make stocastic\n",
    "adjmat_proj_stoc = adjmat_proj / adjmat_proj.sum(axis = 1)\n",
    "print(adjmat_proj_stoc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "special-gabriel",
   "metadata": {},
   "source": [
    "# Projections of suspect graph\n",
    " Projections of suspect graph  \n",
    " From the BCC extracted only the suspect edges and created a new graph that is not connected but still bipartite. From there projected onto people nodes.\n",
    "- [ ] Figure out how to keep crime name as edge attribute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "after-telescope",
   "metadata": {},
   "source": [
    "## Extract suspect edges\n",
    "Created a new graph with only suspects to do further analysis (try projections)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-doctor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT IS THIS ONE????\n",
    "# selected just the suspect edges\n",
    "suspect_edges = [(p,c) for p,c,e in G0.edges(data=True) if e['role'] == 'Suspect']\n",
    "#print(suspect_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "overhead-spring",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting all nodes that have suspect edges\n",
    "sus_nodes = pd.DataFrame(suspect_edges, columns=['node', 'crime']).iloc[:,0].unique()\n",
    "sus_nodes = set(sus_nodes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "senior-ghost",
   "metadata": {},
   "source": [
    "## Suspect subgraph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-extraction",
   "metadata": {},
   "outputs": [],
   "source": [
    "GS = G0.edge_subgraph(suspect_edges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rotary-multimedia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Num. of nodes: {} \\nNum. of edges: {} \\nIs bipartite? {} \\nIs connected? {}'.format(\n",
    "        GS.number_of_nodes(), \n",
    "        GS.number_of_edges(), \n",
    "        nx.is_bipartite(GS),\n",
    "        nx.is_connected(GS)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "educational-cotton",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pos = nx.spring_layout(GS, seed=42)\n",
    "nx.draw(GS, pos, node_size=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "original-practice",
   "metadata": {},
   "source": [
    "## Projection on to suspect nodes (people)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tight-triple",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.is_bipartite(GS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the people and crime nodes\n",
    "people_nodes_GS = {n for n, d in GS.nodes(data=True) if d[\"bipartite\"] == 0}\n",
    "crime_nodes_GS = set(GS) - people_nodes_GS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "confidential-memphis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# projection of suspect graph\n",
    "weighted_suspect_graph = nx.bipartite.weighted_projected_graph(GS, people_nodes_GS, ratio=False)\n",
    "#list(weighted_suspect_graph.edges(data=True))\n",
    "# weight is the number of shared neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-google",
   "metadata": {},
   "outputs": [],
   "source": [
    "# give the number of shared neighbors\n",
    "weights = list(nx.get_edge_attributes(weighted_suspect_graph, 'weight').values())\n",
    "\n",
    "# plot weights\n",
    "labels, counts = np.unique(weights, return_counts=True)\n",
    "plt.bar(labels, counts, align='center', log=True)\n",
    "plt.gca().set_xticks(labels)\n",
    "plt.title(\"Number of Shared Neighborns on Suspect Projection Network\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.xlabel(\"Number of Shared Neighbors\")\n",
    "\n",
    "#plt.hist(weights, bins = 10, log=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fancy-rendering",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_spring(weighted_suspect_graph, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-canada",
   "metadata": {},
   "outputs": [],
   "source": [
    "# WHAT IS THIS?????\n",
    "from collections import defaultdict\n",
    "con = defaultdict(int)\n",
    "for a,b in weighted_suspect_graph.edges:\n",
    "    con[a] += 1\n",
    "    con[b] += 1\n",
    "#print(sorted(con.values(), reverse = True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "imperial-individual",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    'Num. of nodes: {} \\nNum. of edges: {} \\nIs bipartite? {} \\nIs connected? {}'.format(\n",
    "        weighted_suspect_graph.number_of_nodes(), \n",
    "        weighted_suspect_graph.number_of_edges(), \n",
    "        nx.is_bipartite(weighted_suspect_graph),\n",
    "        nx.is_connected(weighted_suspect_graph)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "electronic-carnival",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.number_connected_components(weighted_suspect_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "close-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components in the graph sorted in descendent order\n",
    "sort_sus = sorted(nx.connected_components(weighted_suspect_graph), key=len, reverse=True)\n",
    "\n",
    "# Selecting the biggest component\n",
    "GS2 = weighted_suspect_graph.subgraph(sort_sus[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exceptional-venture",
   "metadata": {},
   "outputs": [],
   "source": [
    "ew = nx.get_edge_attributes(GS2, 'weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "experienced-great",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = {\n",
    "    'node_color': 'orange',\n",
    "    'node_size': 30,\n",
    "    'edge_color': 'gray', \n",
    "    # 'linewidths': 1, \n",
    "    'width': list(ew.values()),\n",
    "    'font_size': 8,\n",
    "    'with_labels': False\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hindu-ribbon",
   "metadata": {},
   "outputs": [],
   "source": [
    "l = nx.shell_layout(GS2)\n",
    "nx.draw(GS2, l, **options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "measured-scottish",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.draw_kamada_kawai(GS2, **options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "general-warren",
   "metadata": {},
   "source": [
    "# Find LCC from Projection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "skilled-commodity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Components in the graph sorted in descendent order\n",
    "P_Gcc = sorted(nx.connected_components(GS), key=len, reverse=True)\n",
    "\n",
    "# Selecting the biggest component\n",
    "P_G0 = GS.subgraph(P_Gcc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "finite-galaxy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of nodes for each component and number of components\n",
    "comp_list = []\n",
    "for comp in P_Gcc:\n",
    "    comp_len = len(comp)\n",
    "    comp_list.append(comp_len)\n",
    "\n",
    "print(f'Number of nodes for each component:\\n{comp_list} \\n\\nNumber of components: {len(comp_list)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modified-costs",
   "metadata": {},
   "source": [
    "# Adjacency Matrix\n",
    "\n",
    "Central Place Theory.\n",
    "Chapter 29 in the book.\n",
    "\n",
    "The Adjacency matrix of our Projected LCC looks like a discrete model. Which imposes a strict core-periphery structure. This also does not seem to show any smaller communities extending off of the core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-arthur",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the degrees of the nodes\n",
    "degree = dict(P_G0.degree)\n",
    "\n",
    "# Figure out which node is of which type\n",
    "rows, cols = nx.algorithms.bipartite.sets(P_G0)\n",
    "\n",
    "# Sort rows and columns according to their degree\n",
    "rows = {n: degree[n] for n in rows}\n",
    "cols = {n: degree[n] for n in cols}\n",
    "rows = [n[0] for n in sorted(rows.items(), key = lambda x : x[1], reverse = True)]\n",
    "cols = [n[0] for n in sorted(cols.items(), key = lambda x : x[1], reverse = True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stylish-strain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the biadjacency matrix with the properly sorted rows and columns\n",
    "B = nx.algorithms.bipartite.matrix.biadjacency_matrix(P_G0, rows, column_order = cols)\n",
    "plt.figure(figsize=(4,8))\n",
    "plt.spy(B, markersize = 3)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "republican-outreach",
   "metadata": {},
   "source": [
    "# Finding Which Nodes Connect to Which Cores\n",
    "\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.core.core_number.html#networkx.algorithms.core.core_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nuclear-quarter",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This finds the cores of the entire network, and counts how many nodes are connected to each core\n",
    "c_whole = nx.algorithms.core.core_number(G)\n",
    "c_whole_count = Counter(c_whole.values())\n",
    "c_whole_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "devoted-cyprus",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This finds the cores of the projected network of suspects\n",
    "c = nx.algorithms.core.core_number(P_G0)\n",
    "count_c = Counter(c.values())\n",
    "print(count_c)\n",
    "print(f\"Core 1 has {count_c[1]} Nodes attached \\nCore 2 has {count_c[2]} Nodes Attached\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "marine-judges",
   "metadata": {},
   "source": [
    "# Degree Assortativity\n",
    "Draw the degree assortativity plots of this network using first an **edge-centric strategy**.<br>\n",
    "Based on Exercise 27.1<br>\n",
    "https://www.networkatlas.eu/exercise.htm?c=27&e=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "limited-mattress",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the degrees of the nodes from the LCC\n",
    "degrees = dict(G0.degree)\n",
    "\n",
    "# Edge-centric plot\n",
    "edge_assort = [(degrees[e[0]], degrees[e[1]]) for e in G0.edges]\n",
    "edge_assort = pd.DataFrame(data = edge_assort, columns = (\"k1\", \"k2\"))\n",
    "edge_assort = edge_assort.groupby(by = [\"k1\", \"k2\"]).size().reset_index().rename(columns = {0: \"count\"}).sort_values(by = \"count\")\n",
    "\n",
    "# Taking the log count of edges with nodes with the same degrees\n",
    "edge_assort[\"count\"] = np.log(edge_assort[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "delayed-evans",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the edge-centric plot for the LCC\n",
    "ax = plt.gca()\n",
    "plt.scatter(edge_assort[\"k1\"], edge_assort[\"k2\"], c = edge_assort[\"count\"], cmap = \"Reds\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "plt.colorbar(ax=ax,label=\"# Of Edges\")\n",
    "plt.xlabel(\"Node Degree\")\n",
    "plt.ylabel(\"Node Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informative-grade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now do the same for the LCC of the Projected Suspect Network to compare\n",
    "# Get the degrees of the nodes from the LCC\n",
    "degrees2 = dict(P_G0.degree)\n",
    "\n",
    "# Edge-centric plot\n",
    "edge_assort = [(degrees2[e[0]], degrees2[e[1]]) for e in P_G0.edges]\n",
    "edge_assort = pd.DataFrame(data = edge_assort, columns = (\"k1\", \"k2\"))\n",
    "edge_assort = edge_assort.groupby(by = [\"k1\", \"k2\"]).size().reset_index().rename(columns = {0: \"count\"}).sort_values(by = \"count\")\n",
    "\n",
    "# Taking the log count of edges with nodes with the same degrees\n",
    "edge_assort[\"count\"] = np.log(edge_assort[\"count\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "white-madagascar",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the edge-centric plot for the LCC of the Projected Network\n",
    "\n",
    "ax = plt.gca()\n",
    "plt.scatter(edge_assort[\"k1\"], edge_assort[\"k2\"], c = edge_assort[\"count\"], cmap = \"Reds\")\n",
    "#ax.set_yscale(\"log\")\n",
    "#ax.set_xscale(\"log\")\n",
    "plt.colorbar(ax=ax, label=\"# Of Edges\")\n",
    "plt.xlabel(\"Node Degree\")\n",
    "plt.ylabel(\"Node Degree\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-recipe",
   "metadata": {},
   "source": [
    "### *This plot does not seem to visualize a high possibility of degree assortivity. (As that would be visualized with perhaps a lot of points plotted on the off diagonal, whereas here there are barely any).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "peripheral-brake",
   "metadata": {},
   "source": [
    "# Degree Assortativity\n",
    "Using a node-centric approach. We plot each node and compare a node’s degree with the average degree of its neighbors. In a **degree assortative** network, we expect to see a positive correlation: the more connections the node has, the more connections, on average, its neighbors have.\n",
    "\n",
    "This also means that you should perform a power fit. The exponent of such a fit tells you whether the network is degree assortative (if it’s positive), disassortative (if it’s negative). \n",
    "\n",
    "It seems the LCC is disassortative: users with few friends likely attach to hubs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "threaded-narrow",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two vectors with the degree of a node in one, and the average neighbor degree in the other\n",
    "node_assort = [(degrees[n], degrees[n2]) for n in G0.nodes for n2 in G0.neighbors(n)]\n",
    "node_assort = pd.DataFrame(data = node_assort, columns = (\"k\", \"neigh_k\"))\n",
    "node_assort = node_assort.groupby(by = \"k\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driving-fellowship",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the node-centric plot\n",
    "ax = plt.gca()\n",
    "plt.scatter(node_assort[\"k\"], node_assort[\"neigh_k\"])\n",
    "#ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "furnished-jaguar",
   "metadata": {},
   "source": [
    "### Degree Assortativity Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amateur-utilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two vectors with the degrees at each endpoints of edges\n",
    "x1 = []\n",
    "y1 = []\n",
    "for e in G0.edges:\n",
    "    x1.append(degrees[e[0]])\n",
    "    y1.append(degrees[e[1]])\n",
    "    x1.append(degrees[e[1]])\n",
    "    y1.append(degrees[e[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "still-royalty",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Edge-centric assortativity is the pearson correlation of such vectors\n",
    "r, p = pearsonr(x1, y1)\n",
    "print(\"Degree assortativity coefficient: %1.4f (p-value < 0.001? %s)\" % (r, p < 0.001))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compatible-territory",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node-centric assortativity is the power-fit of such vectors\n",
    "slope, intercept, r_value, p_value, std_err = linregress(np.log(node_assort[\"k\"]), np.log(node_assort[\"neigh_k\"]))\n",
    "print(\"Degree assortativity coefficient: %1.4f (p-value < 0.001? %s)\" % (slope, p_value < 0.001))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patent-single",
   "metadata": {},
   "source": [
    "## Check LCC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "authorized-buddy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First checking with the LCC of the Entire Network\n",
    "# Get the degrees of the nodes from the LCC\n",
    "degrees3 = dict(G0.degree)\n",
    "\n",
    "# Let's make a new figure\n",
    "plt.clf()\n",
    "\n",
    "# Node-centric plot\n",
    "node_assort = [(degrees3[n], degrees3[n2]) for n in G0.nodes for n2 in G0.neighbors(n)]\n",
    "node_assort = pd.DataFrame(data = node_assort, columns = (\"k\", \"neigh_k\"))\n",
    "node_assort = node_assort.groupby(by = \"k\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "banner-india",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the node-centric plot\n",
    "ax = plt.gca()\n",
    "plt.scatter(node_assort[\"k\"], node_assort[\"neigh_k\"])\n",
    "#ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"Node Degree\")\n",
    "plt.ylabel(\"Average Degree of a Node's Neighbor's\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-argument",
   "metadata": {},
   "source": [
    "Compute degree assortativity of graph.\n",
    "\n",
    "Assortativity measures the similarity of connections in the graph with respect to the node degree.\n",
    "\n",
    "https://networkx.org/documentation/stable/reference/algorithms/generated/networkx.algorithms.assortativity.degree_pearson_correlation_coefficient.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demographic-spectacular",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = nx.degree_pearson_correlation_coefficient(G0)\n",
    "print(f\"{r:3.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "immune-advancement",
   "metadata": {},
   "source": [
    "## Check LCC of Projected Suspect Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "smooth-scotland",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now checking with the LCC of the Projected Suspect Network\n",
    "# Get the degrees of the nodes from the P_LCC\n",
    "degrees = dict(P_G0.degree)\n",
    "\n",
    "# Let's make a new figure\n",
    "plt.clf()\n",
    "\n",
    "# Node-centric plot\n",
    "node_assort = [(degrees[n], degrees[n2]) for n in P_G0.nodes for n2 in P_G0.neighbors(n)]\n",
    "node_assort = pd.DataFrame(data = node_assort, columns = (\"k\", \"neigh_k\"))\n",
    "node_assort = node_assort.groupby(by = \"k\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bored-north",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the node-centric plot\n",
    "ax = plt.gca()\n",
    "plt.scatter(node_assort[\"k\"], node_assort[\"neigh_k\"])\n",
    "#ax.set_yscale(\"log\")\n",
    "ax.set_xscale(\"log\")\n",
    "plt.xlabel(\"Node Degree\")\n",
    "plt.ylabel(\"Average Degree of a Node's Neighbor's\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "suspected-invention",
   "metadata": {},
   "outputs": [],
   "source": [
    "r = nx.degree_pearson_correlation_coefficient(P_G0)\n",
    "print(f\"{r:3.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incorporated-regulation",
   "metadata": {},
   "source": [
    "# Checking for Assortativity in Categorical Attributes (Homophily)\n",
    "\n",
    "### *Assortativity measures the similarity of connections in the graph with respect to the given attribute. Perfect assortativity is 1, while perfect disassortativity is -1. The whole network is quite disassortative.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "random-library",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativity coefficient of the Entire Network\n",
    "nx.attribute_assortativity_coefficient(G, \"sex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "leading-cameroon",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assortativity coefficient of the LCC\n",
    "nx.attribute_assortativity_coefficient(G0, \"sex\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-tattoo",
   "metadata": {},
   "source": [
    "# Configuration Model\n",
    "\n",
    "### This is used to compare what we find. Is what we find special for our network? Or is it something that would happen randomly?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surprising-chain",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A bipartite based on the entire network\n",
    "CM = nx.algorithms.bipartite.generators.configuration_model(e_p, e_c)\n",
    "#nx.draw(CM, node_size=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-abortion",
   "metadata": {},
   "outputs": [],
   "source": [
    "CM_top_nodes = {n for n, d in CM.nodes(data=True) if d[\"bipartite\"] == 0} # people\n",
    "CM_bottom_nodes = set(CM) - CM_top_nodes      # crimes\n",
    "\n",
    "# Average Clustering, Density, and Global Clustering\n",
    "print('Density CM: {} \\nAverage Clustering CM: {} \\nGlobal Clustering CM: {}'.format(\n",
    "        round(bipartite.density(CM, CM_top_nodes),5), \n",
    "        round(bipartite.average_clustering(CM),5),\n",
    "        round(bipartite.robins_alexander_clustering(CM), 4)\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disabled-tamil",
   "metadata": {},
   "outputs": [],
   "source": [
    "closeness_CM = bipartite.closeness_centrality(CM, CM_top_nodes)\n",
    "fig, ax = plt.subplots(1,1, figsize=(5,4))\n",
    "ax.set_title('Closeness Centrality of Configuration Model')\n",
    "ax.hist(closeness_CM.values(), bins=550)\n",
    "ax.set_xlim(.11,.31)\n",
    "ax.set_ylim(0,75)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-attention",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary of Top Nodes and Their Degree\n",
    "CM_top_degree = list(CM.degree(CM_top_nodes))\n",
    "CM_top_degree = pd.DataFrame(CM_top_degree, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Dictionary of Top Node Degrees and Their Frequencies\n",
    "CM_top_degree_cnt = Counter(dict(CM.degree(CM_top_nodes)).values())\n",
    "CM_top_degree_cnt = pd.DataFrame(list(CM_top_degree_cnt.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "CM_top_degree_cnt[\"p(k)\"] = CM_top_degree_cnt[\"p(k)\"]/(CM_top_degree_cnt[\"p(k)\"].sum())\n",
    "\n",
    "# Dictionary of Bottom Nodes and Their Degree\n",
    "CM_bottom_degree = list(CM.degree(CM_bottom_nodes))\n",
    "CM_bottom_degree = pd.DataFrame(CM_bottom_degree, columns = (\"Node\", \"Degree\")).sort_values(by = \"Node\")\n",
    "\n",
    "# Dictionary of Bottom Node Degrees and Their Frequencies\n",
    "CM_bottom_degree_cnt = Counter(dict(CM.degree(CM_bottom_nodes)).values())\n",
    "CM_bottom_degree_cnt = pd.DataFrame(list(CM_bottom_degree_cnt.items()), columns = (\"Degree (k)\", \"p(k)\")).sort_values(by = \"Degree (k)\")\n",
    "CM_bottom_degree_cnt[\"p(k)\"] = CM_bottom_degree_cnt[\"p(k)\"]/(CM_bottom_degree_cnt[\"p(k)\"].sum())\n",
    "\n",
    "# This includes the entire network\n",
    "fig, ax = plt.subplots(figsize=(10,5))\n",
    "ax = plt.gca()\n",
    "CM_top_degree_cnt.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", color = \"#fb00ff\", ax = ax, alpha=0.9, s=200)\n",
    "CM_bottom_degree_cnt.plot(kind = \"scatter\", x = \"Degree (k)\", y = \"p(k)\", color = \"#ff9d00\", ax = ax, alpha=0.7, s=200)\n",
    "ax.legend([\"Top nodes\",\"People nodes\"])\n",
    "plt.xticks(np.arange(0, 26, step=1))\n",
    "plt.title(\"Degree Distributions per Node Category\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "going-sandwich",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "82a91a22060084f3e8dbe72ba49a002eddd687e7ed0e7d249946fe1b995c1ddc"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
